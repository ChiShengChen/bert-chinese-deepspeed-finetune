#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¾®èª¿æ¨¡å‹çš„æ¨ç†è…³æœ¬
ç”¨æ–¼è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹é€²è¡Œå•ç­”æ¨ç†
"""

import torch
from transformers import AutoTokenizer, AutoModelForMaskedLM
import argparse
import os

def load_model(model_path, device=None):
    """
    è¼‰å…¥å¾®èª¿å¾Œçš„æ¨¡å‹å’Œ tokenizer
    
    Args:
        model_path: æ¨¡å‹è·¯å¾‘
        device: è¨­å‚™ (None æ™‚è‡ªå‹•æª¢æ¸¬)
    
    Returns:
        model, tokenizer, device
    """
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    print(f"ğŸ“¦ æ­£åœ¨è¼‰å…¥æ¨¡å‹å¾: {model_path}")
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è¨­å‚™: {device}")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {model_path}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForMaskedLM.from_pretrained(model_path)
        model.to(device)
        model.eval()
        print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼\n")
        return model, tokenizer, device
    except Exception as e:
        raise Exception(f"è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")


def predict_mask(model, tokenizer, prompt, device, top_k=5):
    """
    ä½¿ç”¨ BERT æ¨¡å‹é æ¸¬ [MASK] ä½ç½®çš„è©å½™
    
    Args:
        model: è¼‰å…¥çš„æ¨¡å‹
        tokenizer: tokenizer
        prompt: è¼¸å…¥æ–‡æœ¬ï¼ˆæœƒè‡ªå‹•æ·»åŠ  [MASK]ï¼‰
        device: è¨­å‚™
        top_k: è¿”å›å‰ k å€‹é æ¸¬çµæœ
    
    Returns:
        list: é æ¸¬çš„è©å½™åˆ—è¡¨
    """
    # ç¢ºä¿ prompt åŒ…å« [MASK]
    if tokenizer.mask_token not in prompt:
        prompt = prompt + tokenizer.mask_token
    
    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]
    
    if len(mask_token_index) == 0:
        return ["âš ï¸ æœªæ‰¾åˆ° [MASK] token"]
    
    # æ¨ç†
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
    
    # ç²å– [MASK] ä½ç½®çš„ logits
    mask_token_logits = logits[0, mask_token_index[0], :]
    
    # ç²å– top_k é æ¸¬
    top_k_ids = torch.topk(mask_token_logits, top_k, dim=0).indices.tolist()
    predicted_tokens = [tokenizer.decode([idx]).strip() for idx in top_k_ids]
    
    return predicted_tokens


def qa_inference(model, tokenizer, question, options, device, top_k=3):
    """
    å•ç­”æ¨ç†ï¼šæ ¹æ“šå•é¡Œå’Œé¸é …ï¼Œé æ¸¬æœ€å¯èƒ½çš„ç­”æ¡ˆ
    
    Args:
        model: è¼‰å…¥çš„æ¨¡å‹
        tokenizer: tokenizer
        question: å•é¡Œæ–‡æœ¬
        options: é¸é …å­—å…¸ï¼Œæ ¼å¼å¦‚ {"A": "é¸é …A", "B": "é¸é …B", ...}
        device: è¨­å‚™
        top_k: è¿”å›å‰ k å€‹æœ€å¯èƒ½çš„ç­”æ¡ˆ
    
    Returns:
        list: æœ€å¯èƒ½çš„ç­”æ¡ˆåˆ—è¡¨ï¼ˆæŒ‰æ¦‚ç‡æ’åºï¼‰
    """
    results = []
    
    for label, option_text in options.items():
        # æ§‹å»º prompt: å•é¡Œ + [MASK] + é¸é …
        prompt = f"{question} {tokenizer.mask_token} {option_text}"
        
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]
        
        if len(mask_token_index) == 0:
            continue
        
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
        
        # ç²å– [MASK] ä½ç½®çš„ logits
        mask_logits = logits[0, mask_token_index[0], :]
        
        # è¨ˆç®—é¸é …æ–‡æœ¬çš„ token çš„å¹³å‡åˆ†æ•¸
        option_tokens = tokenizer.encode(option_text, add_special_tokens=False)
        if len(option_tokens) > 0:
            # å–ç¬¬ä¸€å€‹ token çš„åˆ†æ•¸ä½œç‚ºä»£è¡¨
            score = mask_logits[option_tokens[0]].item()
            results.append((label, option_text, score))
    
    # æŒ‰åˆ†æ•¸æ’åº
    results.sort(key=lambda x: x[2], reverse=True)
    
    return results[:top_k]


def interactive_mode(model, tokenizer, device):
    """
    äº’å‹•æ¨¡å¼ï¼šæŒçºŒæ¥æ”¶ç”¨æˆ¶è¼¸å…¥ä¸¦é€²è¡Œæ¨ç†
    """
    print("\n" + "="*60)
    print("ğŸ¤– BERT å¾®èª¿æ¨¡å‹æ¨ç†æ¨¡å¼")
    print("="*60)
    print("æç¤ºï¼š")
    print("  - è¼¸å…¥å•é¡Œæ–‡æœ¬ï¼Œæœƒè‡ªå‹•åœ¨æœ«å°¾æ·»åŠ  [MASK]")
    print("  - è¼¸å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("  - è¼¸å…¥ 'qa' é€²å…¥å•ç­”æ¨¡å¼")
    print("="*60 + "\n")
    
    while True:
        try:
            user_input = input("ğŸ’¬ è«‹è¼¸å…¥å•é¡Œ: ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è¦‹ï¼")
                break
            
            if user_input.lower() == 'qa':
                # å•ç­”æ¨¡å¼
                question = input("â“ å•é¡Œ: ").strip()
                print("ğŸ“ é¸é …ï¼ˆæ ¼å¼ï¼šA:é¸é …A B:é¸é …B C:é¸é …C D:é¸é …Dï¼‰")
                options_input = input("é¸é …: ").strip()
                
                # è§£æé¸é …
                options = {}
                for opt in options_input.split():
                    if ':' in opt:
                        label, text = opt.split(':', 1)
                        options[label.strip()] = text.strip()
                
                if options:
                    results = qa_inference(model, tokenizer, question, options, device)
                    print("\nğŸ¯ é æ¸¬çµæœï¼ˆæŒ‰å¯èƒ½æ€§æ’åºï¼‰:")
                    for i, (label, text, score) in enumerate(results, 1):
                        print(f"  {i}. {label}: {text} (åˆ†æ•¸: {score:.4f})")
                print()
                continue
            
            # æ¨™æº– MASK é æ¸¬æ¨¡å¼
            predictions = predict_mask(model, tokenizer, user_input, device, top_k=5)
            
            print("\nğŸ”® é æ¸¬çµæœï¼ˆTop 5ï¼‰:")
            for i, pred in enumerate(predictions, 1):
                print(f"  {i}. {pred}")
            print()
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è¦‹ï¼")
            break
        except Exception as e:
            print(f"âŒ éŒ¯èª¤: {e}\n")


def main():
    parser = argparse.ArgumentParser(description="å¾®èª¿æ¨¡å‹æ¨ç†è…³æœ¬")
    parser.add_argument(
        "--model_path",
        type=str,
        default="./my_bert_finetuned_model_hf_format",
        help="æ¨¡å‹è·¯å¾‘ï¼ˆé è¨­: ./my_bert_finetuned_model_hf_formatï¼‰"
    )
    parser.add_argument(
        "--cpu",
        action="store_true",
        help="å¼·åˆ¶ä½¿ç”¨ CPU"
    )
    parser.add_argument(
        "--prompt",
        type=str,
        default=None,
        help="å–®æ¬¡æ¨ç†æ¨¡å¼ï¼šç›´æ¥æä¾›å•é¡Œæ–‡æœ¬"
    )
    parser.add_argument(
        "--top_k",
        type=int,
        default=5,
        help="è¿”å›å‰ k å€‹é æ¸¬çµæœï¼ˆé è¨­: 5ï¼‰"
    )
    
    args = parser.parse_args()
    
    # è¨­ç½®è¨­å‚™
    if args.cpu:
        device = torch.device("cpu")
    else:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # è¼‰å…¥æ¨¡å‹
    try:
        model, tokenizer, device = load_model(args.model_path, device)
    except Exception as e:
        print(f"âŒ ç„¡æ³•è¼‰å…¥æ¨¡å‹: {e}")
        return
    
    # å–®æ¬¡æ¨ç†æ¨¡å¼
    if args.prompt:
        predictions = predict_mask(model, tokenizer, args.prompt, device, top_k=args.top_k)
        print(f"\nå•é¡Œ: {args.prompt}")
        print(f"\né æ¸¬çµæœï¼ˆTop {args.top_k}ï¼‰:")
        for i, pred in enumerate(predictions, 1):
            print(f"  {i}. {pred}")
    else:
        # äº’å‹•æ¨¡å¼
        interactive_mode(model, tokenizer, device)


if __name__ == "__main__":
    main()

