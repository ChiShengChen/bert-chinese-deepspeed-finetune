# -*- coding: utf-8 -*-
"""ã€Œfine_tuning_llm.ipynbã€çš„å‰¯æœ¬

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UVPEvEssFZ7tCAnb11jUHyeQkJgRAT0r
"""

# !pip install datasets huggingface_hub fsspec
# !pip install  deepspeed transformers

# æª¢æŸ¥ cuda ç‰ˆæœ¬
# !nvcc --version

"""# DeepSpeed
- https://www.deepspeed.ai/getting-started/
- https://medium.com/@minh.hoque/how-to-train-large-language-models-9e0a56538e22
"""

# è«‹æ ¹æ“šæ‚¨çš„ CUDA ç‰ˆæœ¬é¸æ“‡åˆé©çš„æŒ‡ä»¤
# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu125
# # å®‰è£ MPI (ç”¨æ–¼ DeepSpeed é€šè¨Šå¾Œç«¯):
# !apt-get update -y
# !apt-get install -y libopenmpi-dev openmpi-bin
# !pip install mpi4py
# # å®‰è£ CUDA ç·¨è­¯å·¥å…· (ç”¨æ–¼ DeepSpeed JIT ç·¨è­¯è‡ªè¨‚ CUDA æ ¸å¿ƒ)
# !apt-get install -y cuda-nvcc-12-5  # or correct version if needed

"""## Generate the benchmark QA"""

from datasets import load_dataset
import random

def generate_qa_benchmark(random_seed:int=100, field:str="engineering_math", dataset:str="test"):
    qa_benchmark = []

    # Load the dataset
    ds = load_dataset('ikala/tmmluplus', field, split=dataset) # dataset : train/ val/ test

    # Get 30 random samples
    samples = random.sample(list(ds), random_seed)

    for i, s in enumerate(samples, 1):
        question_data = {
            "id": i,
            "question": s["question"],
            "options": {
                "A": s["A"],
                "B": s["B"],
                "C": s["C"],
                "D": s["D"]
            },
            "answer": s["answer"]
        }
        qa_benchmark.append(question_data)
    return  qa_benchmark

qa_benchmark = generate_qa_benchmark(random_seed=100, dataset="test")
qa_benchmark

"""## Startup the fine-tuning with deepspeed framework"""

import torch
import deepspeed
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForMaskedLM,BertForMaskedLM
from torch.utils.data import DataLoader, TensorDataset, random_split
from datasets import load_dataset
import argparse
import os
import math

# add some arguments for training
parser = argparse.ArgumentParser(description="Training script.")
parser.add_argument("--cpu", action="store_true", help="If passed, will train on the CPU.")
parser.add_argument("--load_dir", type=str, default="", help="The checkpoint loading path")
parser.add_argument("--ckpt_id", type=str, default="", help="The checkpoint index")
parser.add_argument("--save_dir", type=str, default="./checkpoints", help="The checkpoint saving path")
# æœ¬åœ°é‹è¡Œæ™‚ï¼Œå¦‚æœæ²’æœ‰å‘½ä»¤è¡Œåƒæ•¸ï¼Œä½¿ç”¨é»˜èªå€¼
import sys
if len(sys.argv) == 1:
    # æ²’æœ‰å‘½ä»¤è¡Œåƒæ•¸æ™‚ï¼Œä½¿ç”¨ç©ºåˆ—è¡¨é¿å…è§£æéŒ¯èª¤
    args = parser.parse_args(args=[])
else:
    args = parser.parse_args()

# å‰µå»ºå„²å­˜checkpoints è³‡æ–™å¤¾ï¼ˆä½¿ç”¨æœ¬åœ°è·¯å¾‘ï¼‰
CHECKPOINT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "checkpoints")
# è‹¥è³‡æ–™å¤¾ä¸å­˜åœ¨ï¼Œè‡ªå‹•å»ºç«‹
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
# æ›´æ–° args.save_dir
args.save_dir = CHECKPOINT_DIR

# 1. DeepSpeed Configuration (Replace with your actual ds_config.json or parameters)
config_params = {
    "train_batch_size": 32,
    "gradient_accumulation_steps": 1,
    "optimizer": {
        "type": "Adam",
        "params": {
            "lr": 1e-4,
            "betas": [0.9, 0.999],
            "eps": 1e-9,
            "weight_decay": 3e-7
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 1e-5,
            "warmup_num_steps": 100
        }
    },
    "fp16": {
        "enabled": False # Or False, if not using mixed precision (æœ‰æ™‚ gradient scaling æ²’è¨­å¥½ï¼Œå°è‡´æ¢¯åº¦å¾ˆå° â†’ æ›´æ–°å¹…åº¦å¹¾ä¹ 0)
    },
    "zero_optimization": {
        "stage": 0 # Or 1, 2, 3 depending on your ZeRO optimization level
    }
}


def collect_fn(batch):
    pass


# --- Data Preparation (Replace with your actual data loading and preprocessing) ---
def get_dataset(tokenizer, num_samples=100, max_length=50, test_ratio=0.05,val_ratio=0.25, seed=42):
    input_text= []
    task_list = [
                'traditional_chinese_medicine_clinical_medicine', 'clinical_psychology', 'technical', 'culinary_skills', 'mechanical', 'logic_reasoning', 'real_estate',
                'general_principles_of_law', 'finance_banking', 'anti_money_laundering', 'ttqav2', 'marketing_management', 'business_management', 'organic_chemistry', 'advance_chemistry',
                'physics', 'secondary_physics', 'human_behavior', 'national_protection', 'jce_humanities', 'politic_science', 'agriculture', 'official_document_management',
                'financial_analysis', 'pharmacy', 'educational_psychology', 'statistics_and_machine_learning', 'management_accounting', 'introduction_to_law', 'computer_science', 'veterinary_pathology',
                'accounting', 'fire_science', 'optometry', 'insurance_studies', 'pharmacology', 'taxation', 'trust_practice', 'geography_of_taiwan', 'physical_education', 'auditing', 'administrative_law',
                # 'education_(profession_level)', 'economics', 'veterinary_pharmacology', 'nautical_science', 'occupational_therapy_for_psychological_disorders',
                # 'basic_medical_science', 'macroeconomics', 'trade', 'chinese_language_and_literature', 'tve_design', 'junior_science_exam', 'junior_math_exam', 'junior_chinese_exam',
                # 'junior_social_studies', 'tve_mathematics', 'tve_chinese_language', 'tve_natural_sciences', 'junior_chemistry', 'music', 'education', 'three_principles_of_people',
                'taiwanese_hokkien'
                ]
    for id, task in enumerate(task_list):
        task_data = load_dataset('ikala/tmmluplus', task)['train']
        if id == 0:
            # ç¬¬ä¸€æ¬¡å»ºç«‹ key ç‚ºæ¬„ä½åç¨±
            features = [col.strip() for col in task_data.column_names]
        context = [task_data[col] for col in task_data.column_names]
        transposed = list(zip(*context))
        for row in transposed:
            question_dict = {key: value for key, value in zip(features, row)}
            # formatted_qa.append(question_dict)
            input_text.append("".join(f"{key}:{question_dict[key]}\n" for key in features))

    n = len(input_text)
    # åˆ‡åˆ† 5% æ¸¬è©¦é›†è³‡æ–™
    rng = random.Random(seed)
    all_idx = list(range(n))
    rng.shuffle(all_idx)   # éš¨æ©Ÿæ‰“äº‚

    n_test = max(1, math.floor(n * test_ratio))      # è‡³å°‘ 1 ç­†ï¼Œé¿å…ç©ºæ¸¬è©¦é›†
    n_val  = max(1, math.floor(n * val_ratio))
    n_train = n - n_test - n_val

    train_idx, val_idx, test_idx = all_idx[:n_train], all_idx[n_train:n_train+n_val], all_idx[n_train+n_val:]

    train_text = [input_text[i] for i in range(n) if i in train_idx]
    val_text   = [input_text[i] for i in range(n) if i in val_idx]
    test_text  = [input_text[i] for i in range(n) if i in test_idx]
    print("è³‡æ–™æ•¸é‡ï¼š", len(train_text), len(val_text), len(test_text))
    print(f"è¨“ç·´è³‡æ–™:{train_text}")

    def make_dataset(texts):
        # Tokenize all texts (input_texts is list)
        enc = tokenizer(texts, return_tensors="pt", padding="max_length",
                        truncation=True, max_length=max_length)
        # Create labels (for language modeling, labels are typically the input_ids shifted)
        labels = enc.input_ids.clone()
        labels[labels == tokenizer.pad_token_id] = -100
        return TensorDataset(enc.input_ids, enc.attention_mask, labels)
    return make_dataset(train_text), make_dataset(val_text), test_text

model_name = 'bert-base-chinese'
tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    print("add pandding")
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
# model.resize_token_embeddings(len(tokenizer)) # Resize embeddings if new tokens were added (special_tokens)
model = AutoModelForMaskedLM.from_pretrained(model_name)

tokenizer

# 2. Load Model and Tokenizer
model_name = 'bert-base-chinese' # You can choose other GPT-2 variants like 'gpt2-medium'
# tokenizer = GPT2Tokenizer.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
# Add a padding token if it doesn't exist (GPT-2 usually doesn't have one by default)
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
# model = GPT2LMHeadModel.from_pretrained(model_name)
# model.resize_token_embeddings(len(tokenizer)) # Resize embeddings if new tokens were added (special_tokens)
model = AutoModelForMaskedLM.from_pretrained(model_name)
print("Mask token:", tokenizer.mask_token)
print("Mask token id:", tokenizer.mask_token_id)
# --- Prepare Dataset ---
train_data, val_data, test_data = get_dataset(tokenizer)

# Build up the train / val / test dataloader
train_loader = DataLoader(train_data, batch_size=config_params["train_batch_size"], shuffle=True)
val_loader = DataLoader(val_data, batch_size=config_params["train_batch_size"], shuffle=False)

# save test data
test_data

import re
import json

result = []
for item in test_data:
    # æ“·å–é¡Œç›®èˆ‡é¸é …
    q_match = re.search(r"question:(.*?)answer:", item, re.S)
    # æ“·å–æ­£ç¢ºç­”æ¡ˆä»£è™Ÿ
    ans_match = re.search(r"answer:([A-D])", item)

    if not q_match or not ans_match:
        continue

    question_block = q_match.group(1).strip()
    correct_label = ans_match.group(1).strip()

    # æ‰¾å‡ºå°æ‡‰æ–‡å­—å…§å®¹
    option_match = re.search(rf"{correct_label}:(.*?)\n", item)
    correct_answer = option_match.group(1).strip() if option_match else ""

    result.append({
        "question": question_block,
        "answer": correct_answer
    })

# è¼¸å‡ºæˆ JSONï¼ˆä¿å­˜åˆ°å°ˆæ¡ˆç›®éŒ„ï¼‰
test_qa_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "test_qa_data.json")
with open(test_qa_path, "w", encoding="utf-8") as f:
    json.dump(result, f, ensure_ascii=False, indent=4)

print(f"âœ… å·²è¼¸å‡º test_qa_data.json è‡³ {test_qa_path}")

print(len(val_loader))
print(len(train_loader))

print(len(test_data))
print(test_data[0])

# 3. Initialize DeepSpeed Engine
# Make sure deepspeed is installed and ds_config.json is configured or use config_params
# æ³¨æ„ï¼šDeepSpeed é€šå¸¸éœ€è¦é€šé deepspeed å‘½ä»¤å•Ÿå‹•ï¼Œä½†ä¹Ÿå¯ä»¥å–®æ©Ÿé‹è¡Œ
try:
    model_engine, optimizer, _, scheduler = deepspeed.initialize(
        model=model,
        model_parameters=model.parameters(),
        config_params=config_params # or use `config="path/to/ds_config.json"`
    )
    print(f"Using DeepSpeed ZeRO Stage: {model_engine.zero_optimization_stage()}")
    print(f"FP16 enabled: {model_engine.fp16_enabled()}")
except Exception as e:
    print(f"âš ï¸ DeepSpeed åˆå§‹åŒ–å¤±æ•—: {e}")
    print("å˜—è©¦ä½¿ç”¨æ¨™æº– PyTorch è¨“ç·´...")
    # å¦‚æœ DeepSpeed åˆå§‹åŒ–å¤±æ•—ï¼Œä½¿ç”¨æ¨™æº– PyTorch å„ªåŒ–å™¨
    import torch.optim as optim
    # è¨­ç½®è¨­å‚™
    device = torch.device("cuda" if torch.cuda.is_available() and not args.cpu else "cpu")
    model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-9, weight_decay=3e-7)
    from torch.optim.lr_scheduler import LambdaLR
    scheduler = LambdaLR(optimizer, lr_lambda=lambda step: min(1.0, step / 100) * (1e-5 / 1e-4))
    model_engine = model
    print(f"ä½¿ç”¨æ¨™æº– PyTorch è¨“ç·´ï¼Œè¨­å‚™: {device}")

# 4. Training Loop
num_epochs = 40 # Example number of epochs
save_interval =  20

#load checkpoint
if args.load_dir and args.ckpt_id:
    try:
        if hasattr(model_engine, 'load_checkpoint'):
            _, client_sd = model_engine.load_checkpoint(args.load_dir, args.ckpt_id)
            step = client_sd.get('step', 0)
            print(f"âœ… å·²è¼‰å…¥æª¢æŸ¥é»: {args.load_dir}/{args.ckpt_id}")
        else:
            print("âš ï¸ ç•¶å‰æ¨¡å¼ä¸æ”¯æŒè¼‰å…¥æª¢æŸ¥é»")
    except Exception as e:
        print(f"âš ï¸ è¼‰å…¥æª¢æŸ¥é»å¤±æ•—: {e}")

# spit dataser with train / validattion / test
loss_history = []

for epoch in range(num_epochs):
    print(f"Epoch {epoch+1}/{num_epochs}")
    model_engine.train()
    total_loss = 0 # initailize the loss
    for step, batch in enumerate(train_loader):
        input_ids, attention_mask, labels = batch
        # transfer the calculations in
        # ç²å–è¨­å‚™ï¼ˆæ”¯æŒå–®GPUã€å¤šGPUå’ŒCPUï¼‰
        try:
            device = next(model_engine.parameters()).device
        except (StopIteration, AttributeError):
            device = torch.device("cuda" if torch.cuda.is_available() and not args.cpu else "cpu")
        input_ids = input_ids.to(device)
        labels = labels.to(device)
        attention_mask = attention_mask.to(device)

        outputs = model_engine(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        # loss calculations
        t_loss = outputs.loss
        # back prop
        if hasattr(model_engine, 'backward'):
            # DeepSpeed æ¨¡å¼
            model_engine.backward(t_loss)
            model_engine.step() # Includes optimizer.step() and scheduler.step()
        else:
            # æ¨™æº– PyTorch æ¨¡å¼
            optimizer.zero_grad()
            t_loss.backward()
            optimizer.step()
            scheduler.step()
        total_loss += t_loss.item()
        if step % save_interval == 0: # Print loss every 10 steps
          print(f"Step {step}/{len(train_loader)}, Loss: {t_loss.item()}")
    avg_train_loss = total_loss / len(train_loader)
    print(f"Average Training Loss: {avg_train_loss:.4f}")

    # --- Evaluation (Optional, add your evaluation logic here) ---
    model_engine.eval()
    val_loss = 0
    with torch.no_grad():
        for val_step, batch in enumerate(val_loader):
            input_ids, attention_mask,labels = batch
            # ç²å–è¨­å‚™ï¼ˆæ”¯æŒå–®GPUã€å¤šGPUå’ŒCPUï¼‰
            try:
                device = next(model_engine.parameters()).device
            except (StopIteration, AttributeError):
                device = torch.device("cuda" if torch.cuda.is_available() and not args.cpu else "cpu")
            input_ids = input_ids.to(device)
            labels = labels.to(device)
            attention_mask = attention_mask.to(device)

            outputs = model_engine(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            v_loss = outputs.loss.item()
            val_loss += v_loss
            # åªåœ¨æ¯å€‹ epoch çµæŸæ™‚è¨˜éŒ„ä¸€æ¬¡é©—è­‰æå¤±
            if val_step == len(val_loader) - 1:
                loss_history.append(val_loss / len(val_loader))
            # save checkpoint
            if val_step % save_interval:
              # 5. Saving Checkpoints (DeepSpeed handles this internally based on config, but you can also save manually)
              # å®šç¾© client_stateï¼Œå„²å­˜ä½ æƒ³è¦çš„è¨“ç·´è³‡è¨Š
              client_state = {
                  "step": val_step,
                  "val_loss": val_loss
              }
              ckpt_id = f"step{val_step}"
              # model_engine.save_checkpoint(
              #     save_dir=args.save_dir,
              #     tag=ckpt_id,
              #     client_state=client_state
              # )
              # print(f"âœ… Checkpoint saved at {ckpt_id}")
              #model_engine.save_checkpoint(args.save_dir) # This creates a directory
    avg_val_loss = val_loss / len(val_loader)
    print(f"Average Validation Loss: {avg_val_loss:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 4))
plt.plot(loss_history, label='Validation Loss')
plt.xlabel("Step")
plt.ylabel("Loss")
plt.title("Validation Loss Curve")
plt.legend()
plt.grid(True)
# ä¿å­˜åœ–ç‰‡è€Œä¸æ˜¯é¡¯ç¤ºï¼ˆæœ¬åœ°ç’°å¢ƒå¯èƒ½æ²’æœ‰é¡¯ç¤ºå™¨ï¼‰
# ä¿å­˜åˆ°å°ˆæ¡ˆç›®éŒ„
loss_curve_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "validation_loss_curve.png")
plt.savefig(loss_curve_path, dpi=150, bbox_inches='tight')
print(f"âœ… é©—è­‰æå¤±æ›²ç·šå·²ä¿å­˜è‡³ {loss_curve_path}")
# å¦‚æœç’°å¢ƒæ”¯æŒé¡¯ç¤ºï¼Œå¯ä»¥å–æ¶ˆè¨»é‡‹ä¸‹é¢é€™è¡Œ
# plt.show()
plt.close()

"""##  (Zero Redundancy Optimizer) ZeRO
It is a powerful set of memory optimization techniques that enable effective training of large models with trillions of parameters, such as GPT-2 and Turing-NLG 17B.

æ˜¯ DeepSpeed æä¾›çš„è¨“ç·´å„ªåŒ–çš„æ ¸å¿ƒï¼Œå®ƒæ˜¯ä¸€å¥—å‡å°‘åˆ†å¸ƒå¼æ¨¡å‹è®­ç»ƒæ‰€éœ€å†…å­˜é‡çš„æŠ€è¡“ã€‚
æ•¸æ“šä¸¦è¡Œ(Data Parallelism)ç­–ç•¥æœƒå°‡æ¨¡å‹è¤‡è£½åˆ°å¤š gpu è¨­å‚™ä¸­ï¼Œä½†é¡¯ç„¶æœƒé€ æˆä¸å¿…è¦çš„è³‡æºå†—é¤˜
deepSpeed è¨­ç½®äº†ä¸‰ç¨® stage:\n
- Stage 0ï¼šä¸æ¡ç”¨ä»»ä½•å†…å­˜å„ªåŒ–æ–¹æ¡ˆï¼Œä¹Ÿå°±æ˜¯æ™®é€š DDP
- Stage 1ï¼šOptimizer State Partitioning
- Stage 2ï¼šGradient Partitioning
- Stage 3ï¼šParameter Partitioning




"""

# To save the final model for Hugging Face Transformers compatibility:
save_directory = os.path.join(os.path.dirname(os.path.abspath(__file__)), "my_bert_finetuned_model_hf_format")
os.makedirs(save_directory, exist_ok=True)

try:
    if hasattr(model_engine, 'zero_optimization_stage') and model_engine.zero_optimization_stage() == 3:
        # If using ZeRO Stage 3, need to consolidate weights first on rank 0
        if hasattr(model_engine, 'global_rank') and model_engine.global_rank == 0:
            # model_engine.module is the original Hugging Face model
            model_engine.module.save_pretrained(save_directory)
            tokenizer.save_pretrained(save_directory)
            print(f"Model and tokenizer saved to {save_directory} (ZeRO Stage 3 consolidated from rank 0)")
    elif hasattr(model_engine, 'global_rank') and model_engine.global_rank == 0:
        # For ZeRO stages 0, 1, 2 or no ZeRO, save from rank 0
        model_engine.module.save_pretrained(save_directory)
        tokenizer.save_pretrained(save_directory)
        print(f"Model and tokenizer saved by rank 0 to {save_directory} (ZeRO Stage < 3 or no ZeRO)")
    elif hasattr(model_engine, 'module'):
        # DeepSpeed wrapper with module attribute
        model_engine.module.save_pretrained(save_directory)
        tokenizer.save_pretrained(save_directory)
        print(f"Model and tokenizer saved to {save_directory} (DeepSpeed with module attribute)")
    else:
        # Standard PyTorch model (not wrapped by DeepSpeed)
        model_engine.save_pretrained(save_directory)
        tokenizer.save_pretrained(save_directory)
        print(f"Model and tokenizer saved to {save_directory} (Standard PyTorch model)")
except Exception as e:
    print(f"âš ï¸ ä¿å­˜æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    print(f"å˜—è©¦ç›´æ¥ä¿å­˜...")
    try:
        # æœ€å¾Œå˜—è©¦ï¼šç›´æ¥ä¿å­˜
        if hasattr(model_engine, 'module'):
            model_engine.module.save_pretrained(save_directory)
        else:
            model_engine.save_pretrained(save_directory)
        tokenizer.save_pretrained(save_directory)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³ {save_directory}")
    except Exception as e2:
        print(f"âŒ ä¿å­˜å¤±æ•—: {e2}")

print(f"Training complete. Fine-tuned model and tokenizer attempted to be saved. Check logs above for status and path: {save_directory}.")

# To run this script:
# deepspeed gpt2_deepspeed_finetune.py --deepspeed_config ds_config.json (if using a file)
# Or if config_params is embedded:
# deepspeed gpt2_deepspeed_finetune.py

"""## Chat with fine-tuning model

"""

from transformers import AutoTokenizer, AutoModelForMaskedLM
import torch

def chat_with_tuning_llm(prompt, max_length=1000):
    # Adjust this path if your fine-tuned model is saved elsewhere
    # ä½¿ç”¨èˆ‡ä¿å­˜æ™‚ç›¸åŒçš„è·¯å¾‘é‚è¼¯
    model_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "my_bert_finetuned_model_hf_format")
    # Check if CUDA is available and set the device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    # --- Load Fine-tuned Model and Tokenizer ---
    print(f"Loading tokenizer from: {model_path}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForMaskedLM.from_pretrained(model_path)
        model.to(device)
        model.eval()  # Set the model to evaluation mode
    except Exception as e:
        print(f"Error loading tokenizer: {e}")
        return
    print("\nModel and tokenizer loaded successfully.\n")
    # --- Interact with the LLM---
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    with torch.no_grad():
        #
        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.pad_token_id)
        resp = tokenizer.decode(output[0], skip_special_tokens=True)
    return resp

def chat_with_tuning_llm(prompt, top_k=200):
    """
    Args:
        prompt (str): è¼¸å…¥æ–‡å­—å…§å®¹
        top_k (int):

    Returns:
        list: _description_
    """
    try:
        # ä½¿ç”¨èˆ‡ä¿å­˜æ™‚ç›¸åŒçš„è·¯å¾‘é‚è¼¯
        model_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "my_bert_finetuned_model_hf_format")
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForMaskedLM.from_pretrained(model_path).to(device)
        model.eval()
    except Exception as e:
        print(f"Error loading tokenizer: {e}")
        return
    # prompt åŠ å…¥ [MASK]ï¼ŒBERT (MLM æ¨¡å¼ï¼Œå¿…é ˆåŠ å…¥ [MASK])
    # è€Œ GPT (CLM æ¨¡å¼ï¼Œä¸éœ€è¦ [MASK])
    if tokenizer.mask_token not in prompt:
        prompt = prompt + tokenizer.mask_token
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

    mask_token_logits = logits[0, mask_token_index, :]
    top_k_ids = torch.topk(mask_token_logits, top_k, dim=1).indices[0].tolist()
    predicted_tokens = [tokenizer.decode([idx]).strip() for idx in top_k_ids]
    return predicted_tokens

"""## Chat with LLM without tuning"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def general_chat(prompt, max_length=1000):
    accuracy = []
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    try:
        tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
        # æ³¨æ„ï¼šbert-base-chinese æ˜¯ MaskedLM æ¨¡å‹ï¼Œä¸æ˜¯ CausalLM
        # å¦‚æœéœ€è¦ç”Ÿæˆæ–‡æœ¬ï¼Œæ‡‰è©²ä½¿ç”¨ GPT é¡æ¨¡å‹
        model = AutoModelForMaskedLM.from_pretrained("bert-base-chinese")
        model.to(device)
        model.eval()  # Set the model to evaluation mode
    except Exception as e:
        print(f"Error loading model: {e}")
        return None
    print("\nModel and tokenizer loaded successfully.\n")
    # --- Interact with the LLM---
    # BERT æ¨¡å‹ä¸é©åˆç”¨æ–¼æ–‡æœ¬ç”Ÿæˆï¼Œé€™è£¡åƒ…ä½œç¤ºä¾‹
    # å¯¦éš›ä½¿ç”¨æ™‚æ‡‰è©²ä½¿ç”¨ GPT é¡æ¨¡å‹
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    with torch.no_grad():
        # BERT ä¸é©åˆç›´æ¥ç”Ÿæˆï¼Œé€™è£¡åƒ…ä½œç¤ºä¾‹
        # å¯¦éš›æ‡‰è©²ä½¿ç”¨ GPT é¡æ¨¡å‹é€²è¡Œç”Ÿæˆ
        output = model(input_ids=input_ids)
        # ç°¡å–®è¿”å›è¼¸å…¥ï¼ˆå› ç‚º BERT ä¸æ˜¯ç”Ÿæˆæ¨¡å‹ï¼‰
        resp = tokenizer.decode(input_ids[0], skip_special_tokens=True)
    return resp
    # for sub_qa in qa:
    #   print(sub_qa)
    #   if "answer" not in sub_qa:
    #     continue
    #   answer = sub_qa["answer"]
    #   prompt = sub_qa.pop("answer")
    #   prompt += "\n å›ç­”ä»¥ä¸Šå•é¡Œ,çµ¦äºˆä½ çš„ç­”æ¡ˆ,ä¸¦è§£é‡‹ä½ çš„ç­”æ¡ˆ, è¼¸å‡ºç‚ºä»¥ä¸‹æ ¼å¼ï¼š\n [ç­”æ¡ˆ, è§£é‡‹]"
    #   input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    #   with torch.no_grad():
    #       output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.pad_token_id)
    #       response = tokenizer.decode(output[0], skip_special_tokens=True)
    #   if answer in response:
    #      accuracy.append(1)
    #   else:
    #      accuracy.append(0)
    # return accuracy / len(qa)*100

"""## LLM with evaluation"""

EVAL_PROMPT = """
[Question]: {question}
[Reference Answer]: {gold_answer}
[Model A Answer]: {baseline_output}
[Model B Answer]: {finetuned_output}

Act as an impartial judge.
Evaluate both Model A and Model Bâ€™s answers with respect to correctness, relevance, and completeness.
Give a score from 1-10 for each, and declare which is better.
"""

SYS_PROMPT = """


"""

class EvalLLm:
    def __init__(self,
            model_a_resp=None,
            model_b_resp=None,
            ground_truth=None,
            prompt:str=None,
            model_name:str="Qwen/Qwen2.5-3B-Instruct",
            device:str=None):
        self.model = model_name
        self.tokenizer =  AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        if device:
            self.device = torch.device(device)
        else:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model_a_resp = model_a_resp
        self.model_b_resp = model_b_resp
        self.ground_truth = ground_truth
        self.prompt = prompt

    def evaluation(self, question:str):
        """
        ä½¿ç”¨ Qwen æ¨¡å‹è©•ä¼°å…©å€‹å›ç­”çš„è³ªé‡
        
        Args:
            question: å•é¡Œæ–‡æœ¬
        
        Returns:
            list: è©•ä¼°åˆ†æ•¸åˆ—è¡¨ï¼ˆå¦‚æœæˆåŠŸè§£æï¼‰
        """
        # æ§‹å»ºè©•ä¼° prompt
        if self.ground_truth:
            # æœ‰æ¨™æº–ç­”æ¡ˆæ™‚ï¼Œä½¿ç”¨å®Œæ•´çš„è©•ä¼° prompt
            eval_prompt = EVAL_PROMPT.format(
                question=question,
                gold_answer=self.ground_truth,
                baseline_output=str(self.model_a_resp) if self.model_a_resp else "N/A",
                finetuned_output=str(self.model_b_resp) if self.model_b_resp else "N/A"
            )
        else:
            # æ²’æœ‰æ¨™æº–ç­”æ¡ˆæ™‚ï¼Œä½¿ç”¨ç°¡åŒ– prompt
            eval_prompt = f"""
[Question]: {question}
[Model A Answer]: {str(self.model_a_resp) if self.model_a_resp else "N/A"}
[Model B Answer]: {str(self.model_b_resp) if self.model_b_resp else "N/A"}

Act as an impartial judge.
Evaluate both Model A and Model B's answers with respect to correctness, relevance, and completeness.
Give a score from 1-10 for each, and declare which is better.
Format your response as: "Model A: X/10, Model B: Y/10, Better: [A/B]"
"""
        
        # æ§‹å»ºå°è©±æ¶ˆæ¯
        messages = [
            {"role": "system", "content": "You are an impartial judge evaluating model answers."},
            {"role": "user", "content": eval_prompt}
        ]
        
        # æ‡‰ç”¨èŠå¤©æ¨¡æ¿
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # ç·¨ç¢¼è¼¸å…¥
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        # ç”Ÿæˆè©•ä¼°çµæœ
        with torch.no_grad():
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True
            )
        
        # æå–ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆå»é™¤è¼¸å…¥éƒ¨åˆ†ï¼‰
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        # è§£ç¢¼å›ç­”
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        print("æ¨¡å‹å›ç­”ï¼š\n", response)
        
        # å˜—è©¦å¾å›ç­”ä¸­æå–åˆ†æ•¸ï¼ˆæ”¯æŒå¤šç¨®æ ¼å¼ï¼‰
        import re
        scores = []
        
        # æ–¹æ³• 1: æ¨™æº–æ ¼å¼ "Model A: X/10, Model B: Y/10"
        score_pattern1 = r'Model [AB]:\s*(\d+)/10'
        found_scores1 = re.findall(score_pattern1, response, re.IGNORECASE)
        
        # æ–¹æ³• 2: Markdown æ ¼å¼ "**Score: X**" åœ¨ Model A/B éƒ¨åˆ†
        # å…ˆæ‰¾åˆ° Model A å’Œ Model B çš„éƒ¨åˆ†
        model_a_section = re.search(r'Model A[^:]*:.*?(?=Model B|$)', response, re.IGNORECASE | re.DOTALL)
        model_b_section = re.search(r'Model B[^:]*:.*?(?=Model [AC]|$)', response, re.IGNORECASE | re.DOTALL)
        
        scores_a = []
        scores_b = []
        
        if model_a_section:
            # åœ¨ Model A éƒ¨åˆ†æŸ¥æ‰¾åˆ†æ•¸
            a_text = model_a_section.group(0)
            # åŒ¹é… "Score: X" æˆ– "**Score: X**" æˆ– "Score X/10"
            a_patterns = [
                r'\*\*Score:\s*(\d+)\*\*',
                r'Score:\s*(\d+)',
                r'Score\s+(\d+)/10',
                r'(\d+)/10.*?Model A',
                r'Model A.*?(\d+)/10'
            ]
            for pattern in a_patterns:
                matches = re.findall(pattern, a_text, re.IGNORECASE)
                if matches:
                    scores_a.append(int(matches[0]))
                    break
        
        if model_b_section:
            # åœ¨ Model B éƒ¨åˆ†æŸ¥æ‰¾åˆ†æ•¸
            b_text = model_b_section.group(0)
            b_patterns = [
                r'\*\*Score:\s*(\d+)\*\*',
                r'Score:\s*(\d+)',
                r'Score\s+(\d+)/10',
                r'(\d+)/10.*?Model B',
                r'Model B.*?(\d+)/10'
            ]
            for pattern in b_patterns:
                matches = re.findall(pattern, b_text, re.IGNORECASE)
                if matches:
                    scores_b.append(int(matches[0]))
                    break
        
        # æ–¹æ³• 3: ç›´æ¥æŸ¥æ‰¾æ‰€æœ‰ "Score: X" æ ¼å¼ï¼ˆæŒ‰é †åºï¼‰
        all_scores = re.findall(r'(?:Score|åˆ†æ•¸)[:\s]*(\d+)(?:/10)?', response, re.IGNORECASE)
        
        # å„ªå…ˆä½¿ç”¨æ¨™æº–æ ¼å¼
        if len(found_scores1) >= 2:
            scores = [int(s) for s in found_scores1[:2]]
            print(f"âœ… ä½¿ç”¨æ¨™æº–æ ¼å¼è§£æåˆ†æ•¸: Model A = {scores[0]}/10, Model B = {scores[1]}/10")
        # å…¶æ¬¡ä½¿ç”¨ Model A/B éƒ¨åˆ†çš„åˆ†æ•¸
        elif len(scores_a) > 0 and len(scores_b) > 0:
            scores = [scores_a[0], scores_b[0]]
            print(f"âœ… ä½¿ç”¨ Model A/B éƒ¨åˆ†è§£æåˆ†æ•¸: Model A = {scores[0]}/10, Model B = {scores[1]}/10")
        # æœ€å¾Œå˜—è©¦æŒ‰é †åºæå–æ‰€æœ‰åˆ†æ•¸
        elif len(all_scores) >= 2:
            scores = [int(s) for s in all_scores[:2]]
            print(f"âœ… ä½¿ç”¨é †åºè§£æåˆ†æ•¸: Model A = {scores[0]}/10, Model B = {scores[1]}/10")
        # å¦‚æœåªæœ‰ä¸€å€‹åˆ†æ•¸ï¼Œå˜—è©¦æ¨æ–·
        elif len(all_scores) == 1:
            # æª¢æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦æœ‰æ˜ç¢ºçš„ Model A/B æ¨™è¨˜
            if 'Model A' in response[:len(response)//2] and 'Model B' in response[len(response)//2:]:
                # å‡è¨­ç¬¬ä¸€å€‹åˆ†æ•¸æ˜¯ Model Aï¼Œä½†éœ€è¦æ›´å¤šä¿¡æ¯
                print(f"âš ï¸ åªæ‰¾åˆ°ä¸€å€‹åˆ†æ•¸: {all_scores[0]}ï¼Œç„¡æ³•ç¢ºå®š Model A/B çš„å°æ‡‰é—œä¿‚")
            else:
                print(f"âš ï¸ åªæ‰¾åˆ°ä¸€å€‹åˆ†æ•¸: {all_scores[0]}ï¼Œç„¡æ³•è§£æå…©å€‹æ¨¡å‹çš„åˆ†æ•¸")
        else:
            # å¦‚æœç„¡æ³•è§£æï¼Œè¿”å›ç©ºåˆ—è¡¨
            print("âš ï¸ ç„¡æ³•å¾å›ç­”ä¸­è§£æåˆ†æ•¸ï¼Œè«‹æ‰‹å‹•æŸ¥çœ‹è©•ä¼°çµæœ")
            print("ğŸ’¡ æç¤ºï¼šQwen çš„å›ç­”æ ¼å¼å¯èƒ½ä¸ç¬¦åˆé æœŸï¼Œä½†è©•ä¼°å…§å®¹ä»ç„¶æœ‰æ•ˆ")
        
        return scores

    def judgement(self, scores:list, full_score:int=10):
        """
        æ ¹æ“šåˆ†æ•¸åˆ—è¡¨è¨ˆç®—æº–ç¢ºç‡
        
        Args:
            scores: åˆ†æ•¸åˆ—è¡¨ï¼Œé€šå¸¸æ˜¯å–®å€‹å•é¡Œçš„å…©å€‹æ¨¡å‹åˆ†æ•¸ [model_a_score, model_b_score]
            full_score: æ»¿åˆ†ï¼ˆé è¨­ 10ï¼‰
        
        Returns:
            dict: åŒ…å«è©•ä¼°çµæœçš„å­—å…¸
        """
        if not scores:
            print("âš ï¸ scores ç‚ºç©ºï¼Œç„¡æ³•è¨ˆç®—çµ±è¨ˆä¿¡æ¯")
            print("ğŸ’¡ é€™å¯èƒ½æ˜¯å› ç‚º Qwen çš„è©•ä¼°æ ¼å¼ç„¡æ³•è‡ªå‹•è§£æï¼Œä½†è©•ä¼°å…§å®¹ä»ç„¶æœ‰æ•ˆ")
            return {"accuracy": 0.0, "avg_score": 0.0, "total_evaluations": 0}
        
        # å¦‚æœ scores æ˜¯å–®å€‹å•é¡Œçš„å…©å€‹åˆ†æ•¸ [model_a, model_b]
        if len(scores) == 2 and all(isinstance(s, (int, float)) and 0 <= s <= full_score for s in scores):
            model_a_score, model_b_score = int(scores[0]), int(scores[1])
            avg_score = (model_a_score + model_b_score) / 2
            better_model = "B" if model_b_score > model_a_score else "A" if model_a_score > model_b_score else "Tie"
            print(f"ğŸ“Š è©•ä¼°çµæœ:")
            print(f"   Model A: {model_a_score}/{full_score}")
            print(f"   Model B: {model_b_score}/{full_score}")
            print(f"   å¹³å‡åˆ†æ•¸: {avg_score:.2f}/{full_score}")
            print(f"   æ›´å¥½çš„æ¨¡å‹: Model {better_model}")
            return {
                "model_a_score": model_a_score,
                "model_b_score": model_b_score,
                "avg_score": avg_score,
                "better_model": better_model,
                "total_evaluations": 1
            }
        # å¦‚æœ scores æ˜¯å¤šå€‹å•é¡Œçš„åˆ†æ•¸åˆ—è¡¨ï¼ˆæ‰¹é‡è©•ä¼°ï¼‰
        else:
            total = len(scores)
            correct = sum(1 for s in scores if s == full_score)
            accuracy = correct / total * 100 if total > 0 else 0.0
            avg_score = sum(scores) / total if total > 0 else 0.0
            print(f"ğŸ“Š æ‰¹é‡è©•ä¼°çµæœ:")
            print(f"   ç¸½è©•ä¼°æ•¸: {total}")
            print(f"   æº–ç¢ºç‡ (æ»¿åˆ†): {accuracy:.2f}% ({correct}/{total})")
            print(f"   å¹³å‡åˆ†æ•¸: {avg_score:.2f}/{full_score}")
            return {
                "accuracy": accuracy,
                "avg_score": avg_score,
                "total_evaluations": total,
                "correct_count": correct
            }

test_data[0]

import re
def extract_qa_pair(text):
   # æå– question éƒ¨åˆ†ï¼ˆå«é¸é …ï¼‰
  q_part = re.search(r"question:(.*?)(?=answer:)", text, re.S).group(1).strip()
  # æå– answer
  a_part = re.search(r"answer:(.*)", text).group(1).strip()
  print("Question éƒ¨åˆ†ï¼š")
  print(q_part)
  print("\nAnswer éƒ¨åˆ†ï¼š")
  print(a_part)
  return q_part, a_part

def exe_chat(test_qa, max_length=10):
    # parallel with multi-workers
    for qa in test_qa:
      # å¦‚æœ test_qa æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä½¿ç”¨ extract_qa_pair
      if isinstance(qa, str):
          q_part, a_part = extract_qa_pair(qa)
      # å¦‚æœ test_qa æ˜¯å­—å…¸åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
      elif isinstance(qa, dict):
          q_part = qa.get('question', '')
          a_part = qa.get('answer', '')
      else:
          print(f"âš ï¸ ä¸æ”¯æŒçš„æ•¸æ“šæ ¼å¼: {type(qa)}")
          continue
      
      q_part = q_part + "[MASK]"
      tuning_result = chat_with_tuning_llm(q_part, top_k=200)
      print("Tuning LLM Generated text:", tuning_result)
      original_result = general_chat(q_part)
      print("Generated text:", original_result)
      eval_llm = EvalLLm(tuning_result, original_result, a_part, q_part)
      scores = eval_llm.evaluation(q_part)
      eval_llm.judgement(scores)

exe_chat(test_data)

"""## **Reference**
- https://github.com/ChiShengChen/LLM_Finetune_Tutorial/blob/main/DeepSpeed/gpt2_deepspeed_finetune.py
- https://zhuanlan.zhihu.com/p/690690979
- https://www.deepspeed.ai/tutorials/zero/
- https://blog.csdn.net/RandyHan/article/details/132630214
- https://www.kaggle.com/code/afrozs/fine-tuned-gpt2-llm
"""