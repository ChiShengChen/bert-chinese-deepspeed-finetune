# -*- coding: utf-8 -*-
"""「fine_tuning_llm.ipynb」的副本

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UVPEvEssFZ7tCAnb11jUHyeQkJgRAT0r
"""

# !pip install datasets huggingface_hub fsspec
# !pip install  deepspeed transformers

# 檢查 cuda 版本
# !nvcc --version

"""# DeepSpeed
- https://www.deepspeed.ai/getting-started/
- https://medium.com/@minh.hoque/how-to-train-large-language-models-9e0a56538e22
"""

# 請根據您的 CUDA 版本選擇合適的指令
# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu125
# # 安裝 MPI (用於 DeepSpeed 通訊後端):
# !apt-get update -y
# !apt-get install -y libopenmpi-dev openmpi-bin
# !pip install mpi4py
# # 安裝 CUDA 編譯工具 (用於 DeepSpeed JIT 編譯自訂 CUDA 核心)
# !apt-get install -y cuda-nvcc-12-5  # or correct version if needed

"""## Generate the benchmark QA"""

from datasets import load_dataset
import random

def generate_qa_benchmark(random_seed:int=100, field:str="engineering_math", dataset:str="test"):
    qa_benchmark = []

    # Load the dataset
    ds = load_dataset('ikala/tmmluplus', field, split=dataset) # dataset : train/ val/ test

    # Get 30 random samples
    samples = random.sample(list(ds), random_seed)

    for i, s in enumerate(samples, 1):
        question_data = {
            "id": i,
            "question": s["question"],
            "options": {
                "A": s["A"],
                "B": s["B"],
                "C": s["C"],
                "D": s["D"]
            },
            "answer": s["answer"]
        }
        qa_benchmark.append(question_data)
    return  qa_benchmark

qa_benchmark = generate_qa_benchmark(random_seed=100, dataset="test")
qa_benchmark

"""## Startup the fine-tuning with deepspeed framework"""

import torch
import deepspeed
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForMaskedLM,BertForMaskedLM
from torch.utils.data import DataLoader, TensorDataset, random_split
from datasets import load_dataset
import argparse
import os
import math

# add some arguments for training
parser = argparse.ArgumentParser(description="Training script.")
parser.add_argument("--cpu", action="store_true", help="If passed, will train on the CPU.")
parser.add_argument("--load_dir", type=str, default="", help="The checkpoint loading path")
parser.add_argument("--ckpt_id", type=str, default="", help="The checkpoint index")
parser.add_argument("--save_dir", type=str, default="./checkpoints", help="The checkpoint saving path")
# 本地運行時，如果沒有命令行參數，使用默認值
import sys
if len(sys.argv) == 1:
    # 沒有命令行參數時，使用空列表避免解析錯誤
    args = parser.parse_args(args=[])
else:
    args = parser.parse_args()

# 創建儲存checkpoints 資料夾（使用本地路徑）
CHECKPOINT_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "checkpoints")
# 若資料夾不存在，自動建立
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
# 更新 args.save_dir
args.save_dir = CHECKPOINT_DIR

# 1. DeepSpeed Configuration (Replace with your actual ds_config.json or parameters)
config_params = {
    "train_batch_size": 32,
    "gradient_accumulation_steps": 1,
    "optimizer": {
        "type": "Adam",
        "params": {
            "lr": 1e-4,
            "betas": [0.9, 0.999],
            "eps": 1e-9,
            "weight_decay": 3e-7
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 1e-5,
            "warmup_num_steps": 100
        }
    },
    "fp16": {
        "enabled": False # Or False, if not using mixed precision (有時 gradient scaling 沒設好，導致梯度很小 → 更新幅度幾乎 0)
    },
    "zero_optimization": {
        "stage": 0 # Or 1, 2, 3 depending on your ZeRO optimization level
    }
}


def collect_fn(batch):
    pass


# --- Data Preparation (Replace with your actual data loading and preprocessing) ---
def get_dataset(tokenizer, num_samples=100, max_length=50, test_ratio=0.05,val_ratio=0.25, seed=42):
    input_text= []
    task_list = [
                'traditional_chinese_medicine_clinical_medicine', 'clinical_psychology', 'technical', 'culinary_skills', 'mechanical', 'logic_reasoning', 'real_estate',
                'general_principles_of_law', 'finance_banking', 'anti_money_laundering', 'ttqav2', 'marketing_management', 'business_management', 'organic_chemistry', 'advance_chemistry',
                'physics', 'secondary_physics', 'human_behavior', 'national_protection', 'jce_humanities', 'politic_science', 'agriculture', 'official_document_management',
                'financial_analysis', 'pharmacy', 'educational_psychology', 'statistics_and_machine_learning', 'management_accounting', 'introduction_to_law', 'computer_science', 'veterinary_pathology',
                'accounting', 'fire_science', 'optometry', 'insurance_studies', 'pharmacology', 'taxation', 'trust_practice', 'geography_of_taiwan', 'physical_education', 'auditing', 'administrative_law',
                # 'education_(profession_level)', 'economics', 'veterinary_pharmacology', 'nautical_science', 'occupational_therapy_for_psychological_disorders',
                # 'basic_medical_science', 'macroeconomics', 'trade', 'chinese_language_and_literature', 'tve_design', 'junior_science_exam', 'junior_math_exam', 'junior_chinese_exam',
                # 'junior_social_studies', 'tve_mathematics', 'tve_chinese_language', 'tve_natural_sciences', 'junior_chemistry', 'music', 'education', 'three_principles_of_people',
                'taiwanese_hokkien'
                ]
    for id, task in enumerate(task_list):
        task_data = load_dataset('ikala/tmmluplus', task)['train']
        if id == 0:
            # 第一次建立 key 為欄位名稱
            features = [col.strip() for col in task_data.column_names]
        context = [task_data[col] for col in task_data.column_names]
        transposed = list(zip(*context))
        for row in transposed:
            question_dict = {key: value for key, value in zip(features, row)}
            # formatted_qa.append(question_dict)
            input_text.append("".join(f"{key}:{question_dict[key]}\n" for key in features))

    n = len(input_text)
    # 切分 5% 測試集資料
    rng = random.Random(seed)
    all_idx = list(range(n))
    rng.shuffle(all_idx)   # 隨機打亂

    n_test = max(1, math.floor(n * test_ratio))      # 至少 1 筆，避免空測試集
    n_val  = max(1, math.floor(n * val_ratio))
    n_train = n - n_test - n_val

    train_idx, val_idx, test_idx = all_idx[:n_train], all_idx[n_train:n_train+n_val], all_idx[n_train+n_val:]

    train_text = [input_text[i] for i in range(n) if i in train_idx]
    val_text   = [input_text[i] for i in range(n) if i in val_idx]
    test_text  = [input_text[i] for i in range(n) if i in test_idx]
    print("資料數量：", len(train_text), len(val_text), len(test_text))
    print(f"訓練資料:{train_text}")

    def make_dataset(texts):
        # Tokenize all texts (input_texts is list)
        enc = tokenizer(texts, return_tensors="pt", padding="max_length",
                        truncation=True, max_length=max_length)
        # Create labels (for language modeling, labels are typically the input_ids shifted)
        labels = enc.input_ids.clone()
        labels[labels == tokenizer.pad_token_id] = -100
        return TensorDataset(enc.input_ids, enc.attention_mask, labels)
    return make_dataset(train_text), make_dataset(val_text), test_text

model_name = 'bert-base-chinese'
tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    print("add pandding")
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
# model.resize_token_embeddings(len(tokenizer)) # Resize embeddings if new tokens were added (special_tokens)
model = AutoModelForMaskedLM.from_pretrained(model_name)

tokenizer

# 2. Load Model and Tokenizer
model_name = 'bert-base-chinese' # You can choose other GPT-2 variants like 'gpt2-medium'
# tokenizer = GPT2Tokenizer.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
# Add a padding token if it doesn't exist (GPT-2 usually doesn't have one by default)
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
# model = GPT2LMHeadModel.from_pretrained(model_name)
# model.resize_token_embeddings(len(tokenizer)) # Resize embeddings if new tokens were added (special_tokens)
model = AutoModelForMaskedLM.from_pretrained(model_name)
print("Mask token:", tokenizer.mask_token)
print("Mask token id:", tokenizer.mask_token_id)
# --- Prepare Dataset ---
train_data, val_data, test_data = get_dataset(tokenizer)

# Build up the train / val / test dataloader
train_loader = DataLoader(train_data, batch_size=config_params["train_batch_size"], shuffle=True)
val_loader = DataLoader(val_data, batch_size=config_params["train_batch_size"], shuffle=False)

# save test data
test_data

import re
import json

result = []
for item in test_data:
    # 擷取題目與選項
    q_match = re.search(r"question:(.*?)answer:", item, re.S)
    # 擷取正確答案代號
    ans_match = re.search(r"answer:([A-D])", item)

    if not q_match or not ans_match:
        continue

    question_block = q_match.group(1).strip()
    correct_label = ans_match.group(1).strip()

    # 找出對應文字內容
    option_match = re.search(rf"{correct_label}:(.*?)\n", item)
    correct_answer = option_match.group(1).strip() if option_match else ""

    result.append({
        "question": question_block,
        "answer": correct_answer
    })

# 輸出成 JSON（保存到專案目錄）
test_qa_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "test_qa_data.json")
with open(test_qa_path, "w", encoding="utf-8") as f:
    json.dump(result, f, ensure_ascii=False, indent=4)

print(f"✅ 已輸出 test_qa_data.json 至 {test_qa_path}")

print(len(val_loader))
print(len(train_loader))

print(len(test_data))
print(test_data[0])

# 3. Initialize DeepSpeed Engine
# Make sure deepspeed is installed and ds_config.json is configured or use config_params
# 注意：DeepSpeed 通常需要通過 deepspeed 命令啟動，但也可以單機運行
try:
    model_engine, optimizer, _, scheduler = deepspeed.initialize(
        model=model,
        model_parameters=model.parameters(),
        config_params=config_params # or use `config="path/to/ds_config.json"`
    )
    print(f"Using DeepSpeed ZeRO Stage: {model_engine.zero_optimization_stage()}")
    print(f"FP16 enabled: {model_engine.fp16_enabled()}")
except Exception as e:
    print(f"⚠️ DeepSpeed 初始化失敗: {e}")
    print("嘗試使用標準 PyTorch 訓練...")
    # 如果 DeepSpeed 初始化失敗，使用標準 PyTorch 優化器
    import torch.optim as optim
    # 設置設備
    device = torch.device("cuda" if torch.cuda.is_available() and not args.cpu else "cpu")
    model.to(device)
    optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-9, weight_decay=3e-7)
    from torch.optim.lr_scheduler import LambdaLR
    scheduler = LambdaLR(optimizer, lr_lambda=lambda step: min(1.0, step / 100) * (1e-5 / 1e-4))
    model_engine = model
    print(f"使用標準 PyTorch 訓練，設備: {device}")

# 4. Training Loop
num_epochs = 40 # Example number of epochs
save_interval =  20

#load checkpoint
if args.load_dir and args.ckpt_id:
    try:
        if hasattr(model_engine, 'load_checkpoint'):
            _, client_sd = model_engine.load_checkpoint(args.load_dir, args.ckpt_id)
            step = client_sd.get('step', 0)
            print(f"✅ 已載入檢查點: {args.load_dir}/{args.ckpt_id}")
        else:
            print("⚠️ 當前模式不支持載入檢查點")
    except Exception as e:
        print(f"⚠️ 載入檢查點失敗: {e}")

# spit dataser with train / validattion / test
loss_history = []

for epoch in range(num_epochs):
    print(f"Epoch {epoch+1}/{num_epochs}")
    model_engine.train()
    total_loss = 0 # initailize the loss
    for step, batch in enumerate(train_loader):
        input_ids, attention_mask, labels = batch
        # transfer the calculations in
        # 獲取設備（支持單GPU、多GPU和CPU）
        try:
            device = next(model_engine.parameters()).device
        except (StopIteration, AttributeError):
            device = torch.device("cuda" if torch.cuda.is_available() and not args.cpu else "cpu")
        input_ids = input_ids.to(device)
        labels = labels.to(device)
        attention_mask = attention_mask.to(device)

        outputs = model_engine(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        # loss calculations
        t_loss = outputs.loss
        # back prop
        if hasattr(model_engine, 'backward'):
            # DeepSpeed 模式
            model_engine.backward(t_loss)
            model_engine.step() # Includes optimizer.step() and scheduler.step()
        else:
            # 標準 PyTorch 模式
            optimizer.zero_grad()
            t_loss.backward()
            optimizer.step()
            scheduler.step()
        total_loss += t_loss.item()
        if step % save_interval == 0: # Print loss every 10 steps
          print(f"Step {step}/{len(train_loader)}, Loss: {t_loss.item()}")
    avg_train_loss = total_loss / len(train_loader)
    print(f"Average Training Loss: {avg_train_loss:.4f}")

    # --- Evaluation (Optional, add your evaluation logic here) ---
    model_engine.eval()
    val_loss = 0
    with torch.no_grad():
        for val_step, batch in enumerate(val_loader):
            input_ids, attention_mask,labels = batch
            # 獲取設備（支持單GPU、多GPU和CPU）
            try:
                device = next(model_engine.parameters()).device
            except (StopIteration, AttributeError):
                device = torch.device("cuda" if torch.cuda.is_available() and not args.cpu else "cpu")
            input_ids = input_ids.to(device)
            labels = labels.to(device)
            attention_mask = attention_mask.to(device)

            outputs = model_engine(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            v_loss = outputs.loss.item()
            val_loss += v_loss
            # 只在每個 epoch 結束時記錄一次驗證損失
            if val_step == len(val_loader) - 1:
                loss_history.append(val_loss / len(val_loader))
            # save checkpoint
            if val_step % save_interval:
              # 5. Saving Checkpoints (DeepSpeed handles this internally based on config, but you can also save manually)
              # 定義 client_state，儲存你想要的訓練資訊
              client_state = {
                  "step": val_step,
                  "val_loss": val_loss
              }
              ckpt_id = f"step{val_step}"
              # model_engine.save_checkpoint(
              #     save_dir=args.save_dir,
              #     tag=ckpt_id,
              #     client_state=client_state
              # )
              # print(f"✅ Checkpoint saved at {ckpt_id}")
              #model_engine.save_checkpoint(args.save_dir) # This creates a directory
    avg_val_loss = val_loss / len(val_loader)
    print(f"Average Validation Loss: {avg_val_loss:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 4))
plt.plot(loss_history, label='Validation Loss')
plt.xlabel("Step")
plt.ylabel("Loss")
plt.title("Validation Loss Curve")
plt.legend()
plt.grid(True)
# 保存圖片而不是顯示（本地環境可能沒有顯示器）
# 保存到專案目錄
loss_curve_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "validation_loss_curve.png")
plt.savefig(loss_curve_path, dpi=150, bbox_inches='tight')
print(f"✅ 驗證損失曲線已保存至 {loss_curve_path}")
# 如果環境支持顯示，可以取消註釋下面這行
# plt.show()
plt.close()

"""##  (Zero Redundancy Optimizer) ZeRO
It is a powerful set of memory optimization techniques that enable effective training of large models with trillions of parameters, such as GPT-2 and Turing-NLG 17B.

是 DeepSpeed 提供的訓練優化的核心，它是一套减少分布式模型训练所需内存量的技術。
數據並行(Data Parallelism)策略會將模型複製到多 gpu 設備中，但顯然會造成不必要的資源冗餘
deepSpeed 設置了三種 stage:\n
- Stage 0：不採用任何内存優化方案，也就是普通 DDP
- Stage 1：Optimizer State Partitioning
- Stage 2：Gradient Partitioning
- Stage 3：Parameter Partitioning




"""

# To save the final model for Hugging Face Transformers compatibility:
save_directory = os.path.join(os.path.dirname(os.path.abspath(__file__)), "my_bert_finetuned_model_hf_format")
os.makedirs(save_directory, exist_ok=True)

try:
    if hasattr(model_engine, 'zero_optimization_stage') and model_engine.zero_optimization_stage() == 3:
        # If using ZeRO Stage 3, need to consolidate weights first on rank 0
        if hasattr(model_engine, 'global_rank') and model_engine.global_rank == 0:
            # model_engine.module is the original Hugging Face model
            model_engine.module.save_pretrained(save_directory)
            tokenizer.save_pretrained(save_directory)
            print(f"Model and tokenizer saved to {save_directory} (ZeRO Stage 3 consolidated from rank 0)")
    elif hasattr(model_engine, 'global_rank') and model_engine.global_rank == 0:
        # For ZeRO stages 0, 1, 2 or no ZeRO, save from rank 0
        model_engine.module.save_pretrained(save_directory)
        tokenizer.save_pretrained(save_directory)
        print(f"Model and tokenizer saved by rank 0 to {save_directory} (ZeRO Stage < 3 or no ZeRO)")
    elif hasattr(model_engine, 'module'):
        # DeepSpeed wrapper with module attribute
        model_engine.module.save_pretrained(save_directory)
        tokenizer.save_pretrained(save_directory)
        print(f"Model and tokenizer saved to {save_directory} (DeepSpeed with module attribute)")
    else:
        # Standard PyTorch model (not wrapped by DeepSpeed)
        model_engine.save_pretrained(save_directory)
        tokenizer.save_pretrained(save_directory)
        print(f"Model and tokenizer saved to {save_directory} (Standard PyTorch model)")
except Exception as e:
    print(f"⚠️ 保存模型時發生錯誤: {e}")
    print(f"嘗試直接保存...")
    try:
        # 最後嘗試：直接保存
        if hasattr(model_engine, 'module'):
            model_engine.module.save_pretrained(save_directory)
        else:
            model_engine.save_pretrained(save_directory)
        tokenizer.save_pretrained(save_directory)
        print(f"✅ 模型已保存至 {save_directory}")
    except Exception as e2:
        print(f"❌ 保存失敗: {e2}")

print(f"Training complete. Fine-tuned model and tokenizer attempted to be saved. Check logs above for status and path: {save_directory}.")

# To run this script:
# deepspeed gpt2_deepspeed_finetune.py --deepspeed_config ds_config.json (if using a file)
# Or if config_params is embedded:
# deepspeed gpt2_deepspeed_finetune.py

"""## Chat with fine-tuning model

"""

from transformers import AutoTokenizer, AutoModelForMaskedLM
import torch

def chat_with_tuning_llm(prompt, max_length=1000):
    # Adjust this path if your fine-tuned model is saved elsewhere
    # 使用與保存時相同的路徑邏輯
    model_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "my_bert_finetuned_model_hf_format")
    # Check if CUDA is available and set the device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    # --- Load Fine-tuned Model and Tokenizer ---
    print(f"Loading tokenizer from: {model_path}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForMaskedLM.from_pretrained(model_path)
        model.to(device)
        model.eval()  # Set the model to evaluation mode
    except Exception as e:
        print(f"Error loading tokenizer: {e}")
        return
    print("\nModel and tokenizer loaded successfully.\n")
    # --- Interact with the LLM---
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    with torch.no_grad():
        #
        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.pad_token_id)
        resp = tokenizer.decode(output[0], skip_special_tokens=True)
    return resp

def chat_with_tuning_llm(prompt, top_k=200):
    """
    Args:
        prompt (str): 輸入文字內容
        top_k (int):

    Returns:
        list: _description_
    """
    try:
        # 使用與保存時相同的路徑邏輯
        model_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "my_bert_finetuned_model_hf_format")
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForMaskedLM.from_pretrained(model_path).to(device)
        model.eval()
    except Exception as e:
        print(f"Error loading tokenizer: {e}")
        return
    # prompt 加入 [MASK]，BERT (MLM 模式，必須加入 [MASK])
    # 而 GPT (CLM 模式，不需要 [MASK])
    if tokenizer.mask_token not in prompt:
        prompt = prompt + tokenizer.mask_token
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

    mask_token_logits = logits[0, mask_token_index, :]
    top_k_ids = torch.topk(mask_token_logits, top_k, dim=1).indices[0].tolist()
    predicted_tokens = [tokenizer.decode([idx]).strip() for idx in top_k_ids]
    return predicted_tokens

"""## Chat with LLM without tuning"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def general_chat(prompt, max_length=1000):
    accuracy = []
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    try:
        tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
        # 注意：bert-base-chinese 是 MaskedLM 模型，不是 CausalLM
        # 如果需要生成文本，應該使用 GPT 類模型
        model = AutoModelForMaskedLM.from_pretrained("bert-base-chinese")
        model.to(device)
        model.eval()  # Set the model to evaluation mode
    except Exception as e:
        print(f"Error loading model: {e}")
        return None
    print("\nModel and tokenizer loaded successfully.\n")
    # --- Interact with the LLM---
    # BERT 模型不適合用於文本生成，這裡僅作示例
    # 實際使用時應該使用 GPT 類模型
    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    with torch.no_grad():
        # BERT 不適合直接生成，這裡僅作示例
        # 實際應該使用 GPT 類模型進行生成
        output = model(input_ids=input_ids)
        # 簡單返回輸入（因為 BERT 不是生成模型）
        resp = tokenizer.decode(input_ids[0], skip_special_tokens=True)
    return resp
    # for sub_qa in qa:
    #   print(sub_qa)
    #   if "answer" not in sub_qa:
    #     continue
    #   answer = sub_qa["answer"]
    #   prompt = sub_qa.pop("answer")
    #   prompt += "\n 回答以上問題,給予你的答案,並解釋你的答案, 輸出為以下格式：\n [答案, 解釋]"
    #   input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
    #   with torch.no_grad():
    #       output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.pad_token_id)
    #       response = tokenizer.decode(output[0], skip_special_tokens=True)
    #   if answer in response:
    #      accuracy.append(1)
    #   else:
    #      accuracy.append(0)
    # return accuracy / len(qa)*100

"""## LLM with evaluation"""

EVAL_PROMPT = """
[Question]: {question}
[Reference Answer]: {gold_answer}
[Model A Answer]: {baseline_output}
[Model B Answer]: {finetuned_output}

Act as an impartial judge.
Evaluate both Model A and Model B’s answers with respect to correctness, relevance, and completeness.
Give a score from 1-10 for each, and declare which is better.
"""

SYS_PROMPT = """


"""

class EvalLLm:
    def __init__(self,
            model_a_resp=None,
            model_b_resp=None,
            ground_truth=None,
            prompt:str=None,
            model_name:str="Qwen/Qwen2.5-3B-Instruct",
            device:str=None):
        self.model = model_name
        self.tokenizer =  AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        if device:
            self.device = torch.device(device)
        else:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model_a_resp = model_a_resp
        self.model_b_resp = model_b_resp
        self.ground_truth = ground_truth
        self.prompt = prompt

    def evaluation(self, question:str):
        """
        使用 Qwen 模型評估兩個回答的質量
        
        Args:
            question: 問題文本
        
        Returns:
            list: 評估分數列表（如果成功解析）
        """
        # 構建評估 prompt
        if self.ground_truth:
            # 有標準答案時，使用完整的評估 prompt
            eval_prompt = EVAL_PROMPT.format(
                question=question,
                gold_answer=self.ground_truth,
                baseline_output=str(self.model_a_resp) if self.model_a_resp else "N/A",
                finetuned_output=str(self.model_b_resp) if self.model_b_resp else "N/A"
            )
        else:
            # 沒有標準答案時，使用簡化 prompt
            eval_prompt = f"""
[Question]: {question}
[Model A Answer]: {str(self.model_a_resp) if self.model_a_resp else "N/A"}
[Model B Answer]: {str(self.model_b_resp) if self.model_b_resp else "N/A"}

Act as an impartial judge.
Evaluate both Model A and Model B's answers with respect to correctness, relevance, and completeness.
Give a score from 1-10 for each, and declare which is better.
Format your response as: "Model A: X/10, Model B: Y/10, Better: [A/B]"
"""
        
        # 構建對話消息
        messages = [
            {"role": "system", "content": "You are an impartial judge evaluating model answers."},
            {"role": "user", "content": eval_prompt}
        ]
        
        # 應用聊天模板
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # 編碼輸入
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        # 生成評估結果
        with torch.no_grad():
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=512,
                temperature=0.7,
                do_sample=True
            )
        
        # 提取生成的文本（去除輸入部分）
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        # 解碼回答
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        print("模型回答：\n", response)
        
        # 嘗試從回答中提取分數（簡單的正則表達式解析）
        import re
        scores = []
        # 嘗試提取分數，例如 "Model A: 8/10, Model B: 9/10"
        score_pattern = r'Model [AB]:\s*(\d+)/10'
        found_scores = re.findall(score_pattern, response)
        if len(found_scores) >= 2:
            scores = [int(s) for s in found_scores[:2]]
        else:
            # 如果無法解析，返回空列表
            print("⚠️ 無法從回答中解析分數，請手動查看評估結果")
        
        return scores

    def judgement(self, scores:list, full_score:int=10):
        """
        根據分數列表計算準確率
        """
        if not scores:
            print("⚠️ scores 為空，無法計算")
            return 0.0
        accuracy = 0
        correct = sum(1 for s in scores if s == full_score)
        accuracy = correct / len(scores) * 100
        avg_score = sum(scores) / len(scores)
        print(f"Accuracy: {accuracy:.2f}% | 平均信心分數: {avg_score:.2f}/{full_score}")
        return {"accuracy": accuracy, "avg_score": avg_score}

test_data[0]

import re
def extract_qa_pair(text):
   # 提取 question 部分（含選項）
  q_part = re.search(r"question:(.*?)(?=answer:)", text, re.S).group(1).strip()
  # 提取 answer
  a_part = re.search(r"answer:(.*)", text).group(1).strip()
  print("Question 部分：")
  print(q_part)
  print("\nAnswer 部分：")
  print(a_part)
  return q_part, a_part

def exe_chat(test_qa, max_length=10):
    # parallel with multi-workers
    for qa in test_qa:
      # 如果 test_qa 是字符串列表，使用 extract_qa_pair
      if isinstance(qa, str):
          q_part, a_part = extract_qa_pair(qa)
      # 如果 test_qa 是字典列表，直接使用
      elif isinstance(qa, dict):
          q_part = qa.get('question', '')
          a_part = qa.get('answer', '')
      else:
          print(f"⚠️ 不支持的數據格式: {type(qa)}")
          continue
      
      q_part = q_part + "[MASK]"
      tuning_result = chat_with_tuning_llm(q_part, top_k=200)
      print("Tuning LLM Generated text:", tuning_result)
      original_result = general_chat(q_part)
      print("Generated text:", original_result)
      eval_llm = EvalLLm(tuning_result, original_result, a_part, q_part)
      scores = eval_llm.evaluation(q_part)
      eval_llm.judgement(scores)

exe_chat(test_data)

"""## **Reference**
- https://github.com/ChiShengChen/LLM_Finetune_Tutorial/blob/main/DeepSpeed/gpt2_deepspeed_finetune.py
- https://zhuanlan.zhihu.com/p/690690979
- https://www.deepspeed.ai/tutorials/zero/
- https://blog.csdn.net/RandyHan/article/details/132630214
- https://www.kaggle.com/code/afrozs/fine-tuned-gpt2-llm
"""