# BERT ä¸­æ–‡æ¨¡å‹å¾®èª¿å°ˆæ¡ˆ

åŸºæ–¼ DeepSpeed æ¡†æ¶çš„ BERT ä¸­æ–‡æ¨¡å‹å¾®èª¿å°ˆæ¡ˆï¼Œä½¿ç”¨ TMMLU+ å¤šé ˜åŸŸä¸­æ–‡å•ç­”è³‡æ–™é›†é€²è¡Œè¨“ç·´ã€‚

## ğŸ“‹ å°ˆæ¡ˆç°¡ä»‹

æœ¬å°ˆæ¡ˆå¯¦ç¾äº†ä½¿ç”¨ DeepSpeed æ¡†æ¶å° BERT ä¸­æ–‡æ¨¡å‹é€²è¡Œå¾®èª¿ï¼Œæ”¯æ´åœ¨ 40+ å€‹å°ˆæ¥­é ˜åŸŸï¼ˆé†«å­¸ã€æ³•å¾‹ã€é‡‘èã€ç‰©ç†ç­‰ï¼‰çš„ä¸­æ–‡å•ç­”ä»»å‹™ä¸Šé€²è¡Œè¨“ç·´ã€‚å°ˆæ¡ˆå¾ Google Colab é·ç§»è€Œä¾†ï¼Œå·²é©é…æœ¬åœ°é‹è¡Œç’°å¢ƒã€‚

## âœ¨ ä¸»è¦ç‰¹æ€§

- ğŸš€ **DeepSpeed æ”¯æ´**ï¼šä½¿ç”¨ DeepSpeed æ¡†æ¶é€²è¡Œé«˜æ•ˆè¨“ç·´ï¼Œæ”¯æ´ ZeRO å„ªåŒ–
- ğŸ”„ **è‡ªå‹•é™ç´š**ï¼šDeepSpeed ä¸å¯ç”¨æ™‚è‡ªå‹•é™ç´šåˆ°æ¨™æº– PyTorch è¨“ç·´
- ğŸ¯ **å¤šé ˜åŸŸè¨“ç·´**ï¼šæ¶µè“‹ 40+ å€‹ä¸­æ–‡å°ˆæ¥­é ˜åŸŸçŸ¥è­˜
- ğŸ’¾ **æª¢æŸ¥é»ç®¡ç†**ï¼šæ”¯æ´è¨“ç·´æª¢æŸ¥é»çš„ä¿å­˜å’Œè¼‰å…¥
- ğŸ“Š **è¦–è¦ºåŒ–**ï¼šè‡ªå‹•ç”Ÿæˆè¨“ç·´æå¤±æ›²ç·šåœ–
- ğŸ”§ **è¨­å‚™è‡ªé©æ‡‰**ï¼šè‡ªå‹•æª¢æ¸¬ä¸¦ä½¿ç”¨ GPU/CPU
- ğŸ“ **å®Œæ•´è©•ä¼°**ï¼šåŒ…å«æ¨¡å‹è©•ä¼°å’Œå°æ¯”åŠŸèƒ½

## ğŸ› ï¸ ç’°å¢ƒè¦æ±‚

### ç³»çµ±è¦æ±‚
- Python 3.8+
- CUDA 11.0+ (å¯é¸ï¼Œç”¨æ–¼ GPU è¨“ç·´)
- Linux / Windows / macOS

### ä¾è³´åº«

ä¸»è¦ä¾è³´ï¼š
- `torch` >= 1.9.0
- `transformers` >= 4.20.0
- `datasets` >= 2.0.0
- `deepspeed` >= 0.6.0 (å¯é¸)
- `matplotlib` >= 3.3.0
- `numpy` >= 1.20.0

## ğŸ“¦ å®‰è£æ­¥é©Ÿ

### 1. å…‹éš†æˆ–ä¸‹è¼‰å°ˆæ¡ˆ

```bash
cd /path/to/your/project
```

### 2. å»ºç«‹è™›æ“¬ç’°å¢ƒï¼ˆæ¨è–¦ï¼‰

```bash
conda create -n llm_finetune python=3.10
conda activate llm_finetune
```

æˆ–ä½¿ç”¨ venvï¼š

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# æˆ–
venv\Scripts\activate  # Windows
```

### 3. å®‰è£ä¾è³´

```bash
# å®‰è£åŸºç¤ä¾è³´
pip install torch transformers datasets matplotlib numpy

# å®‰è£ DeepSpeed (å¯é¸ï¼Œä½†æ¨è–¦)
pip install deepspeed

# å¦‚æœéœ€è¦ GPU æ”¯æ´ï¼Œæ ¹æ“š CUDA ç‰ˆæœ¬å®‰è£ PyTorch
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### 4. é©—è­‰å®‰è£

```bash
python -c "import torch; import transformers; import deepspeed; print('âœ… æ‰€æœ‰ä¾è³´å®‰è£æˆåŠŸ')"
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬é‹è¡Œ

```bash
# ä½¿ç”¨é è¨­é…ç½®é‹è¡Œï¼ˆè‡ªå‹•æª¢æ¸¬ GPUï¼‰
python fine_tuning_llm_ipynb.py

# å¼·åˆ¶ä½¿ç”¨ CPU
python fine_tuning_llm_ipynb.py --cpu

# æŒ‡å®šæª¢æŸ¥é»ä¿å­˜è·¯å¾‘
python fine_tuning_llm_ipynb.py --save_dir ./my_checkpoints

# å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´
python fine_tuning_llm_ipynb.py --load_dir ./checkpoints --ckpt_id step100
```

### ä½¿ç”¨ DeepSpeed å•Ÿå‹•ï¼ˆæ¨è–¦ï¼‰

```bash
# å–® GPU
deepspeed fine_tuning_llm_ipynb.py

# å¤š GPU
deepspeed --num_gpus=4 fine_tuning_llm_ipynb.py

# ä½¿ç”¨é…ç½®æª”æ¡ˆ
deepspeed --deepspeed_config ds_config.json fine_tuning_llm_ipynb.py
```

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
LLM_example/
â”œâ”€â”€ fine_tuning_llm_ipynb.py    # ä¸»è¨“ç·´è…³æœ¬
â”œâ”€â”€ checkpoints/                 # è¨“ç·´æª¢æŸ¥é»ç›®éŒ„ï¼ˆè‡ªå‹•å»ºç«‹ï¼‰
â”œâ”€â”€ my_bert_finetuned_model_hf_format/  # å¾®èª¿å¾Œçš„æ¨¡å‹ï¼ˆè¨“ç·´å¾Œç”Ÿæˆï¼‰
â”œâ”€â”€ test_qa_data.json            # æ¸¬è©¦è³‡æ–™ JSON æª”æ¡ˆï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰
â”œâ”€â”€ validation_loss_curve.png   # é©—è­‰æå¤±æ›²ç·šåœ–ï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰
â””â”€â”€ README.md                    # æœ¬æª”æ¡ˆ
```

## âš™ï¸ é…ç½®èªªæ˜

### DeepSpeed é…ç½®

åœ¨ç¨‹å¼ç¢¼ä¸­çš„ `config_params` å­—å…¸ä¸­å¯ä»¥èª¿æ•´è¨“ç·´åƒæ•¸ï¼š

```python
config_params = {
    "train_batch_size": 32,              # è¨“ç·´æ‰¹æ¬¡å¤§å°
    "gradient_accumulation_steps": 1,     # æ¢¯åº¦ç´¯ç©æ­¥æ•¸
    "optimizer": {
        "type": "Adam",
        "params": {
            "lr": 1e-4,                   # å­¸ç¿’ç‡
            "betas": [0.9, 0.999],
            "eps": 1e-9,
            "weight_decay": 3e-7
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 1e-5,
            "warmup_num_steps": 100
        }
    },
    "fp16": {
        "enabled": False                  # æ··åˆç²¾åº¦è¨“ç·´
    },
    "zero_optimization": {
        "stage": 0                        # ZeRO å„ªåŒ–éšæ®µ (0, 1, 2, 3)
    }
}
```

### è¨“ç·´åƒæ•¸

- `num_epochs`: è¨“ç·´è¼ªæ•¸ï¼ˆé è¨­ 40ï¼‰
- `save_interval`: ä¿å­˜é–“éš”æ­¥æ•¸ï¼ˆé è¨­ 20ï¼‰
- `train_batch_size`: æ‰¹æ¬¡å¤§å°ï¼ˆé è¨­ 32ï¼‰
- `max_length`: æœ€å¤§åºåˆ—é•·åº¦ï¼ˆé è¨­ 50ï¼‰

### è³‡æ–™é›†é…ç½®

ç¨‹å¼ç¢¼æ”¯æ´å¾ TMMLU+ è³‡æ–™é›†çš„å¤šå€‹é ˜åŸŸè¼‰å…¥è³‡æ–™ï¼ŒåŒ…æ‹¬ï¼š
- é†«å­¸ã€æ³•å¾‹ã€é‡‘èã€ç‰©ç†ã€åŒ–å­¸ç­‰ 40+ å€‹å°ˆæ¥­é ˜åŸŸ
- è‡ªå‹•æŒ‰ 70% / 25% / 5% åŠƒåˆ†è¨“ç·´/é©—è­‰/æ¸¬è©¦é›†

## ğŸ“Š è¼¸å‡ºæª”æ¡ˆèªªæ˜

### 1. æª¢æŸ¥é»æª”æ¡ˆ (`checkpoints/`)
è¨“ç·´éç¨‹ä¸­ä¿å­˜çš„æ¨¡å‹æª¢æŸ¥é»ï¼Œå¯ç”¨æ–¼æ¢å¾©è¨“ç·´ã€‚

### 2. å¾®èª¿æ¨¡å‹ (`my_bert_finetuned_model_hf_format/`)
è¨“ç·´å®Œæˆå¾Œä¿å­˜çš„æ¨¡å‹å’Œ tokenizerï¼Œå¯ç›´æ¥ç”¨æ–¼æ¨ç†ã€‚

### 3. æ¸¬è©¦è³‡æ–™ (`test_qa_data.json`)
å¾æ¸¬è©¦é›†ä¸­æå–çš„çµæ§‹åŒ–å•ç­”è³‡æ–™ã€‚

### 4. æå¤±æ›²ç·š (`validation_loss_curve.png`)
è¨“ç·´éç¨‹ä¸­çš„é©—è­‰æå¤±è¦–è¦ºåŒ–åœ–è¡¨ã€‚

## ğŸ” ç¨‹å¼ç¢¼åŠŸèƒ½æ¨¡çµ„

### 1. è³‡æ–™æº–å‚™
- `generate_qa_benchmark()`: ç”Ÿæˆå•ç­”åŸºæº–æ¸¬è©¦è³‡æ–™
- `get_dataset()`: è¼‰å…¥å’Œé è™•ç†å¤šé ˜åŸŸè³‡æ–™é›†

### 2. æ¨¡å‹è¨“ç·´
- DeepSpeed åˆå§‹åŒ–ï¼ˆå¸¶é™ç´šæ©Ÿåˆ¶ï¼‰
- è¨“ç·´å¾ªç’°ï¼ˆæ”¯æ´æª¢æŸ¥é»ä¿å­˜/è¼‰å…¥ï¼‰
- é©—è­‰è©•ä¼°

### 3. æ¨¡å‹æ¨ç†
- `chat_with_tuning_llm()`: ä½¿ç”¨å¾®èª¿æ¨¡å‹é€²è¡Œæ¨ç†
- `general_chat()`: ä½¿ç”¨åŸå§‹æ¨¡å‹é€²è¡Œæ¨ç†

### 4. æ¨¡å‹è©•ä¼°
- `EvalLLm`: ä½¿ç”¨ LLM è©•ä¼°æ¨¡å‹å›ç­”å“è³ª
- `exe_chat()`: åœ¨æ¸¬è©¦é›†ä¸ŠåŸ·è¡Œå®Œæ•´è©•ä¼°æµç¨‹

## ğŸ› å¸¸è¦‹å•é¡Œ

### Q: DeepSpeed åˆå§‹åŒ–å¤±æ•—æ€éº¼è¾¦ï¼Ÿ
A: ç¨‹å¼ç¢¼æœƒè‡ªå‹•é™ç´šåˆ°æ¨™æº– PyTorch è¨“ç·´ï¼Œç„¡éœ€æ“”å¿ƒã€‚å¦‚æœæƒ³ä½¿ç”¨ DeepSpeedï¼Œè«‹ç¢ºä¿æ­£ç¢ºå®‰è£ï¼š
```bash
pip install deepspeed
```

### Q: è¨˜æ†¶é«”ä¸è¶³æ€éº¼è¾¦ï¼Ÿ
A: å¯ä»¥å˜—è©¦ä»¥ä¸‹æ–¹æ³•ï¼š
1. æ¸›å° `train_batch_size`
2. å¢åŠ  `gradient_accumulation_steps`
3. å•Ÿç”¨ ZeRO Stage 2 æˆ– 3
4. å•Ÿç”¨ FP16 æ··åˆç²¾åº¦è¨“ç·´

### Q: å¦‚ä½•èª¿æ•´è¨“ç·´é ˜åŸŸï¼Ÿ
A: ä¿®æ”¹ `get_dataset()` å‡½æ•¸ä¸­çš„ `task_list` åˆ—è¡¨ï¼Œæ–°å¢æˆ–åˆªé™¤éœ€è¦çš„é ˜åŸŸã€‚

### Q: æ¨¡å‹ä¿å­˜å¤±æ•—ï¼Ÿ
A: æª¢æŸ¥ç£ç¢Ÿç©ºé–“å’Œå¯«å…¥æ¬Šé™ï¼Œç¢ºä¿æœ‰è¶³å¤ çš„å„²å­˜ç©ºé–“ã€‚

### Q: CUDA out of memory éŒ¯èª¤ï¼Ÿ
A: 
1. æ¸›å°æ‰¹æ¬¡å¤§å°
2. ä½¿ç”¨æ¢¯åº¦ç´¯ç©
3. å•Ÿç”¨ ZeRO å„ªåŒ–
4. ä½¿ç”¨ CPU è¨“ç·´ï¼ˆæ–°å¢ `--cpu` åƒæ•¸ï¼‰

## ğŸ“š åƒè€ƒè³‡æ–™

- [DeepSpeed å®˜æ–¹æ–‡ä»¶](https://www.deepspeed.ai/)
- [Transformers æ–‡ä»¶](https://huggingface.co/docs/transformers)
- [TMMLU+ è³‡æ–™é›†](https://huggingface.co/datasets/ikala/tmmluplus)
- [BERT ä¸­æ–‡æ¨¡å‹](https://huggingface.co/bert-base-chinese)

## ğŸ“ è¨“ç·´æµç¨‹èªªæ˜

1. **è³‡æ–™è¼‰å…¥**: å¾ TMMLU+ è³‡æ–™é›†è¼‰å…¥å¤šé ˜åŸŸä¸­æ–‡å•ç­”è³‡æ–™
2. **è³‡æ–™é è™•ç†**: Tokenization å’Œè³‡æ–™é›†åŠƒåˆ†
3. **æ¨¡å‹åˆå§‹åŒ–**: è¼‰å…¥ BERT ä¸­æ–‡é è¨“ç·´æ¨¡å‹
4. **DeepSpeed åˆå§‹åŒ–**: é…ç½®è¨“ç·´å¼•æ“ï¼ˆå¤±æ•—å‰‡é™ç´šï¼‰
5. **è¨“ç·´å¾ªç’°**: 
   - å‰å‘å‚³æ’­
   - æå¤±è¨ˆç®—
   - åå‘å‚³æ’­
   - åƒæ•¸æ›´æ–°
6. **é©—è­‰è©•ä¼°**: æ¯å€‹ epoch çµæŸå¾Œåœ¨é©—è­‰é›†ä¸Šè©•ä¼°
7. **æ¨¡å‹ä¿å­˜**: ä¿å­˜å¾®èª¿å¾Œçš„æ¨¡å‹å’Œ tokenizer
8. **çµæœè¦–è¦ºåŒ–**: ç”Ÿæˆæå¤±æ›²ç·šåœ–

## ğŸ¯ ä½¿ç”¨å ´æ™¯

- ä¸­æ–‡å•ç­”ç³»çµ±é–‹ç™¼
- å¤šé ˜åŸŸçŸ¥è­˜ç†è§£ä»»å‹™
- æ¨¡å‹å¾®èª¿å¯¦é©—
- å°æ¯”å­¸ç¿’ç ”ç©¶
- ä¸­æ–‡ NLP æ‡‰ç”¨é–‹ç™¼

## âš ï¸ æ³¨æ„äº‹é …

1. **é¦–æ¬¡é‹è¡Œ**æœƒä¸‹è¼‰é è¨“ç·´æ¨¡å‹å’Œè³‡æ–™é›†ï¼Œéœ€è¦è¼ƒé•·æ™‚é–“å’Œç¶²è·¯é€£æ¥
2. **è¨“ç·´æ™‚é–“**å–æ±ºæ–¼ç¡¬é«”é…ç½®ï¼ŒGPU è¨“ç·´æœƒé¡¯è‘—åŠ å¿«é€Ÿåº¦
3. **å„²å­˜ç©ºé–“**ï¼šç¢ºä¿æœ‰è¶³å¤ ç©ºé–“å„²å­˜æ¨¡å‹å’Œæª¢æŸ¥é»ï¼ˆç´„ 1-2 GBï¼‰
4. **è¨˜æ†¶é«”éœ€æ±‚**ï¼šå»ºè­°è‡³å°‘ 8GB RAMï¼ŒGPU è¨“ç·´éœ€è¦ 4GB+ é¡¯å­˜

## ğŸ“„ æˆæ¬Šè¨±å¯

æœ¬å°ˆæ¡ˆåŸºæ–¼åŸå§‹ Colab notebook ä¿®æ”¹ï¼Œè«‹åƒè€ƒåŸå§‹å°ˆæ¡ˆçš„æˆæ¬Šè¨±å¯ã€‚

## ğŸ¤ è²¢ç»

æ­¡è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“§ è¯çµ¡æ–¹å¼

å¦‚æœ‰å•é¡Œæˆ–å»ºè­°ï¼Œè«‹é€é Issue å›é¥‹ã€‚

---

**Happy Fine-tuning! ğŸš€**
