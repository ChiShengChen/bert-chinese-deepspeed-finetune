# BERT Chinese Model Fine-tuning Project

**English** | [ÁπÅÈ´î‰∏≠Êñá](README_zh_TW.md)

---

A BERT Chinese model fine-tuning project based on DeepSpeed framework, trained on TMMLU+ multi-domain Chinese Q&A dataset.

## üìã Project Overview

This project implements fine-tuning of BERT Chinese model using DeepSpeed framework, supporting training on 40+ professional domains (medicine, law, finance, physics, etc.) for Chinese Q&A tasks. The project was migrated from Google Colab and adapted for local execution.

## ‚ú® Key Features

- üöÄ **DeepSpeed Support**: Efficient training with DeepSpeed framework, supporting ZeRO optimization
- üîÑ **Auto Fallback**: Automatically falls back to standard PyTorch training when DeepSpeed is unavailable
- üéØ **Multi-domain Training**: Covers 40+ Chinese professional domain knowledge
- üíæ **Auto Model Saving**: Automatically saves model to `my_bert_finetuned_model_hf_format/` after training, ready for inference
- üì¶ **Checkpoint Management**: Supports saving and loading training checkpoints
- üìä **Visualization**: Automatically generates training loss curve
- üîß **Device Adaptive**: Automatically detects and uses GPU/CPU
- üìù **Complete Evaluation**: Includes model evaluation and comparison functions
- ‚ö†Ô∏è **Important Note**: BERT is a Masked Language Model, not suitable for generative chat, but suitable for fill-in-the-blank and multiple-choice Q&A tasks

## üõ†Ô∏è Requirements

### System Requirements
- Python 3.8+
- CUDA 11.0+ (optional, for GPU training)
- Linux / Windows / macOS

### Dependencies

Main dependencies:
- `torch` >= 1.9.0
- `transformers` >= 4.20.0
- `datasets` >= 2.0.0
- `deepspeed` >= 0.6.0 (optional)
- `matplotlib` >= 3.3.0
- `numpy` >= 1.20.0

## üì¶ Installation

### 1. Clone or Download Project

```bash
cd /path/to/your/project
```

### 2. Create Virtual Environment (Recommended)

```bash
conda create -n llm_finetune python=3.10
conda activate llm_finetune
```

Or use venv:

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows
```

### 3. Install Dependencies

```bash
# Install basic dependencies
pip install torch transformers datasets matplotlib numpy

# Install DeepSpeed (optional but recommended)
pip install deepspeed

# For GPU support, install PyTorch according to CUDA version
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### 4. Verify Installation

```bash
python -c "import torch; import transformers; import deepspeed; print('‚úÖ All dependencies installed successfully')"
```

## üöÄ Usage

### Basic Run

```bash
# Run with default configuration (auto-detect GPU)
python fine_tuning_llm_ipynb.py

# Force CPU usage
python fine_tuning_llm_ipynb.py --cpu

# Specify checkpoint save path
python fine_tuning_llm_ipynb.py --save_dir ./my_checkpoints

# Resume training from checkpoint
python fine_tuning_llm_ipynb.py --load_dir ./checkpoints --ckpt_id step100
```

### Using DeepSpeed (Recommended)

```bash
# Single GPU
deepspeed fine_tuning_llm_ipynb.py

# Multiple GPUs
deepspeed --num_gpus=4 fine_tuning_llm_ipynb.py

# With config file
deepspeed --deepspeed_config ds_config.json fine_tuning_llm_ipynb.py
```

### Using Fine-tuned Model for Inference

After training, use `inference.py` script for inference:

```bash
# Interactive mode (recommended)
python inference.py

# Single inference
python inference.py --prompt "‰ªäÂ§©Â§©Ê∞£[MASK]"

# Specify model path
python inference.py --model_path ./my_bert_finetuned_model_hf_format

# Force CPU
python inference.py --cpu

# Custom top-k results
python inference.py --prompt "question text" --top_k 10
```

**Interactive Mode Features:**
- Input question text, automatically predicts [MASK] position vocabulary
- Input `qa` to enter Q&A mode, can compare multiple options
- Input `quit` or `exit` to exit

## üìÅ Project Structure

```
LLM_example/
‚îú‚îÄ‚îÄ fine_tuning_llm_ipynb.py    # Main training script
‚îú‚îÄ‚îÄ inference.py                 # Model inference script
‚îú‚îÄ‚îÄ checkpoints/                 # Training checkpoint directory (auto-created)
‚îú‚îÄ‚îÄ my_bert_finetuned_model_hf_format/  # Fine-tuned model (generated after training)
‚îú‚îÄ‚îÄ test_qa_data.json            # Test data JSON file (auto-generated)
‚îú‚îÄ‚îÄ validation_loss_curve.png   # Validation loss curve (auto-generated)
‚îî‚îÄ‚îÄ README.md                    # This file
```

## ‚öôÔ∏è Configuration

### DeepSpeed Configuration

Adjust training parameters in the `config_params` dictionary in code:

```python
config_params = {
    "train_batch_size": 32,              # Training batch size
    "gradient_accumulation_steps": 1,     # Gradient accumulation steps
    "optimizer": {
        "type": "Adam",
        "params": {
            "lr": 1e-4,                   # Learning rate
            "betas": [0.9, 0.999],
            "eps": 1e-9,
            "weight_decay": 3e-7
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 1e-5,
            "warmup_num_steps": 100
        }
    },
    "fp16": {
        "enabled": False                  # Mixed precision training
    },
    "zero_optimization": {
        "stage": 0                        # ZeRO optimization stage (0, 1, 2, 3)
    }
}
```

### Training Parameters

- `num_epochs`: Number of training epochs (default: 40)
- `save_interval`: Save interval steps (default: 20)
- `train_batch_size`: Batch size (default: 32)
- `max_length`: Maximum sequence length (default: 50)

### Dataset Configuration

Code supports loading data from multiple domains of TMMLU+ dataset, including:
- Medicine, law, finance, physics, chemistry, and 40+ professional domains
- Automatically splits into 70% / 25% / 5% for train/validation/test sets

## üìä Output Files

### 1. Checkpoint Files (`checkpoints/`)
Model checkpoints saved during training, can be used to resume training.

### 2. Fine-tuned Model (`my_bert_finetuned_model_hf_format/`)
**Automatically saved model and tokenizer after training, ready for inference.**

**Model Saving Features:**
- ‚úÖ Model automatically saved to `my_bert_finetuned_model_hf_format/` directory
- ‚úÖ Includes complete model weights and tokenizer configuration
- ‚úÖ Uses Hugging Face format, can be loaded with `from_pretrained()`
- ‚úÖ Supports both DeepSpeed and standard PyTorch modes
- ‚úÖ Code includes loading and inference functions

**‚ö†Ô∏è Important Limitations:**
- **BERT is a Masked Language Model (MLM)**, not a generative model
- **Not suitable** for open-ended conversational chat (like ChatGPT)
- **Suitable** for:
  - Fill-in-the-blank tasks: Predict vocabulary at [MASK] position
  - Multiple-choice Q&A: Compare options to find most likely answer
  - Text understanding and classification tasks
- For true chat functionality, use **GPT-style generative models** (GPT-2, ChatGLM, Qwen, etc.)

### 3. Test Data (`test_qa_data.json`)
Structured Q&A data extracted from test set.

### 4. Loss Curve (`validation_loss_curve.png`)
Visualization chart of validation loss during training.

## üíæ Model Saving and Usage

### Model Saving Features

**‚úÖ Auto Save:**
- After training, model is automatically saved to `my_bert_finetuned_model_hf_format/` directory
- Saved in Hugging Face Transformers standard format
- Includes complete model weights, configuration files, and tokenizer

**‚úÖ Saved Contents:**
- `config.json`: Model configuration
- `pytorch_model.bin` or `model.safetensors`: Model weights
- `tokenizer_config.json`: Tokenizer configuration
- `vocab.txt`: Vocabulary
- Other necessary configuration files

**‚úÖ Loading Method:**
```python
from transformers import AutoTokenizer, AutoModelForMaskedLM

# Load saved model
model_path = "./my_bert_finetuned_model_hf_format"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForMaskedLM.from_pretrained(model_path)
```

### ‚ö†Ô∏è BERT Model Limitations

**Important: BERT is not suitable for generative chat**

#### Why BERT Base Model is Not Suitable for Chat?

**1. Architecture Differences:**

BERT is a **Bidirectional Encoder**:
- Uses **Transformer Encoder** architecture
- Can see entire sequence context (both forward and backward) during training
- Designed for **understanding** text semantics, not **generating** text

Generative models (like GPT) are **Unidirectional Decoders**:
- Use **Transformer Decoder** architecture
- Can only see context before current position (causal masking)
- Designed for **autoregressive generation**, generating next token one by one

**2. Different Training Objectives:**

BERT's training tasks:
- **Masked Language Modeling (MLM)**: Predict single masked vocabulary
- **Next Sentence Prediction (NSP)**: Judge if two sentences are consecutive
- Goal: Learn **bidirectional semantic representations**

Generative model's training tasks:
- **Causal Language Modeling (CLM)**: Predict next vocabulary based on previous ones
- Goal: Learn **autoregressive generation** capability

**3. Technical Limitations:**

BERT's limitations:
- ‚ùå **Cannot autoregressively generate**: No decoder self-attention mechanism
- ‚ùå **Cannot handle sequence generation**: Can only predict single [MASK] position
- ‚ùå **No generation loop**: Cannot generate tokens one by one to form complete answer
- ‚ùå **Bidirectional attention unsuitable for generation**: Should not see "future" information during generation

Generative model's advantages:
- ‚úÖ **Autoregressive generation**: Can generate tokens one by one
- ‚úÖ **Sequence generation capability**: Can generate text of arbitrary length
- ‚úÖ **Causal masking**: Ensures only using already generated content during generation

**4. Practical Application Differences:**

BERT's application scenarios:
```
Input: "‰ªäÂ§©Â§©Ê∞£Âæà[MASK]"
Output: ["Â•Ω", "ÁÜ±", "ÂÜ∑", ...]  # Can only predict single vocabulary
```

Generative model's application scenarios:
```
Input: "‰ªäÂ§©Â§©Ê∞£ÂæàÂ•ΩÔºå"
Output: "‰ªäÂ§©Â§©Ê∞£ÂæàÂ•ΩÔºåÈÅ©ÂêàÂá∫ÈñÄÊï£Ê≠•„ÄÇ"  # Can generate complete sentence
```

**Summary:**
- BERT is a **understanding model**, focused on text understanding and semantic representation
- GPT-style models are **generative models**, focused on text generation and conversation
- Their architectures, training objectives, and application scenarios are completely different
- Therefore BERT is not suitable for chat tasks requiring continuous text generation

**BERT Suitable Tasks:**
- ‚úÖ Fill-in-the-blank: `"‰ªäÂ§©Â§©Ê∞£Âæà[MASK]"` ‚Üí Predict "Â•Ω", "ÁÜ±", etc.
- ‚úÖ Multiple-choice: Compare multiple options to find most likely answer
- ‚úÖ Text classification: Judge text category
- ‚úÖ Q&A understanding: Understand semantic relationship between question and text

**Unsuitable Tasks:**
- ‚ùå Open-ended dialogue: Cannot generate continuous dialogue text
- ‚ùå Long text generation: Cannot autoregressively generate
- ‚ùå Creative writing: Cannot freely create
- ‚ùå Chatbot: Cannot have multi-turn conversations like ChatGPT

**For Chat Functionality, Recommended:**
- Use **GPT-style models** (GPT-2, GPT-3, ChatGLM, Qwen, etc.)
- Use **Causal Language Model** for fine-tuning
- This project's BERT model is mainly for **Q&A understanding** and **fill-in-the-blank tasks**

#### Technical Comparison Table

| Feature | BERT (Encoder) | GPT (Decoder) |
|---------|----------------|----------------|
| **Architecture** | Transformer Encoder | Transformer Decoder |
| **Attention Mechanism** | Bidirectional | Causal (Unidirectional) |
| **Training Task** | MLM + NSP | Causal LM |
| **Generation Capability** | ‚ùå Cannot generate | ‚úÖ Can generate |
| **Understanding Capability** | ‚úÖ Excellent | ‚úÖ Good |
| **Suitable Tasks** | Classification, Understanding, Fill-in-blank | Generation, Dialogue, Creation |
| **Chat Suitability** | ‚ùå Not suitable | ‚úÖ Suitable |

## üêõ FAQ

### Q: What if DeepSpeed initialization fails?
A: Code will automatically fall back to standard PyTorch training, no worries. To use DeepSpeed, ensure proper installation:
```bash
pip install deepspeed
```

### Q: What if running out of memory?
A: Try the following methods:
1. Reduce `train_batch_size`
2. Increase `gradient_accumulation_steps`
3. Enable ZeRO Stage 2 or 3
4. Enable FP16 mixed precision training

### Q: How to adjust training domains?
A: Modify the `task_list` in `get_dataset()` function, add or remove needed domains.

### Q: Model save failed?
A: Check disk space and write permissions, ensure sufficient storage space.

### Q: CUDA out of memory error?
A: 
1. Reduce batch size
2. Use gradient accumulation
3. Enable ZeRO optimization
4. Use CPU training (add `--cpu` parameter)

## üìö References

- [DeepSpeed Official Documentation](https://www.deepspeed.ai/)
- [Transformers Documentation](https://huggingface.co/docs/transformers)
- [TMMLU+ Dataset](https://huggingface.co/datasets/ikala/tmmluplus)
- [BERT Chinese Model](https://huggingface.co/bert-base-chinese)

## ‚ö†Ô∏è Notes

1. **First run** will download pretrained models and datasets, requires longer time and network connection
2. **Training time** depends on hardware configuration, GPU training significantly speeds up
3. **Storage space**: Ensure sufficient space for models and checkpoints (approximately 1-2 GB)
4. **Memory requirements**: Recommend at least 8GB RAM, GPU training needs 4GB+ VRAM
5. **BERT Model Limitations**:
   - BERT is a **Masked Language Model (MLM)**, not a generative model
   - **Not suitable** for open-ended conversational chat (like ChatGPT)
   - **Suitable** for:
     - Fill-in-the-blank tasks (predict vocabulary at [MASK] position)
     - Multiple-choice Q&A (compare option probabilities)
     - Text classification and understanding tasks
   - For true chat functionality, use **GPT-style generative models**

## üìÑ License

This project is modified from original Colab notebook, please refer to original project's license.

## ü§ù Contributing

Welcome to submit Issues and Pull Requests!

## üìß Contact

For questions or suggestions, please provide feedback through Issues.

---

**Happy Fine-tuning! üöÄ**
